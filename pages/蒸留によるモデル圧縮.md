---
title: "蒸留によるモデル圧縮"
---

- 現時点でのハードウェア制約ではデプロイできない
- とはいえ、ハードウェア性能はどんどん上がるんだから、近い将来単なる[[アンサンブル]]で動くようになるのでは

- 教師モデルの出力で生徒モデルを学習
    - one-hotではなくSoftmaxの出力をそのまま使うケース
    - [[ソフトターゲットロス]]
    - [[ラベルスムージング]]に相当

[1503.02531 Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)


- [[Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer]]
    - 生徒モデルが教師モデルの[[注意]]を学ぶ

- [[Born Again Neural Networks]]
    - 教師と生徒で同じモデルを使った場合に生徒の方が性能が良くなる

- [[Deep Mutual Learning]]
    - 生徒同士で教え合う
