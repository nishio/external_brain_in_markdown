
- [[SVM]]は、計算した値f(x)が正か負かで識別する[[二値分類]]アルゴリズム
- 本質的に[[確率モデル]]ではない。しかし[[LIBSVM]]や[[sklearn]]などでは確率推測が実装されている。これはどういう仕組みか？

- [A Note on Platt’s Probabilistic Outputs for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf)
- Platt(2000)
    - ![image](https://gyazo.com/2d12317f6e5d60ce9fe55a6ac142962a/thumb/1000)
    - ラベルを予測するのではなく、確率を知りたいっていう応用がしばしばあるので、Plattが「シグモイドで近似したらいいのでは」と提案しましたよ、という話。
- A, Bはパラメータであり、対数尤度最大化をすることで求める
    - つまり「SVMの出力値をsignに入れるのではなく、その値を説明変数として[[ロジスティック回帰]]を学習」に相当
- 5-foldのクロスバリデーション
    - (4/5のデータでSVMを学習して、残りの1/5に対してf(x)を求め、そのf(x)と正解の関係をロジスティック回帰で学習)
- SVMできれいに分かれてしまう場合などは、当然ロジスティック回帰がステップ関数になってしまう
    - そこでスムージングなどを掛けている

- Scikit-Learnのpredict_probaを使う上での注意点
    - 「確率値を返さないアルゴリズムにロジスティック回帰を重ねて確率値を返させている」ということを理解する。
    - 4/5のデータで5回SVMを学習するわけなので、まあざっくり4～5倍くらい時間がかかる。
    - 確率値を返すようにすることで、SVMの識別精度は上がらない。それを目的とした仕組みではない。
    - パラメータが適切に選ばれているなら、同程度の精度になる。

#機械学習
