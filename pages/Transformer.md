---
title: "Transformer"
---

[[RNN]]なし[[CNN]]なしで[[注意機構]]だけ構成されたTransformerが翻訳タスクで良い成績を出すという報告。
> We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

- Attention Is All You Need [[Łukasz Kaiser et al., arXiv, 2017/06]]
- 2017年
- [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)


解説 2017-12 [http://deeplearning.hatenablog.com/entry/transformer](http://deeplearning.hatenablog.com/entry/transformer)
- [[注意機構]]
    - [[注意機構は辞書オブジェクト]]
    - [[加法注意]]と[[内積注意]]
    - [[ソースターゲット注意]]と[[自己注意]]
    - [[縮小付き内積注意]] (Scaled Dot-Product Attention)

