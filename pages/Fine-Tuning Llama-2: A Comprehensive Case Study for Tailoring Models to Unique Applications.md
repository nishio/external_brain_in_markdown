---
title: "Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications"
---

[[Llama2]]のFine tuningガイド
- [Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications)

ファインチューニングのサービスを提供している会社による自社サービスを使ってのファインチューニングの事例紹介
- 「たくさん試した中で一番上手くいった事例を紹介してる」と考えるべきだろう

Llama-2モデル
- 3つの実際のユースケースで検証
- ファインチューニングによって精度向上


いくつかのニッチなケースではGPT-4よりも優れている
- 非構造化テキストからの機能表現抽出（ViGGO）
- SQL生成(SQL-create-context)
- どちらも7Bで十分
    - 一方で数学理解のタスクGSMに関しては70Bでも全然追いつかない

- 特にLlama-13bでは、関数表現で58％から98％、SQL生成で42％から89％、GSMで28％から47％の精度の向上が見られた。

微調整の基本
3つのタスクすべてにおいて、我々は標準的な全パラメータのファインチューニング技術を使用する。
- モデルは次のトークンを予測するために微調整される
- モデル内のすべてのパラメータは勾配更新の対象となります
- ブロック凍結やLoRAは使わない
- Rayを使えば楽にできるよ(という宣伝)


データをワーカー間でシャード
DeepSpeedでモデルのシャーディング


特殊トークン
- 自然文で指示するのではなく特殊トークンを使ってタスクを構造化している
    - その学習で自然文での指示に対する性能が上がる？そうではなくどうやって変換するかは無視している？
    - 後者っぽいな、何らかの理由で構造化された入力が取得できる場合に、構造を明確に伝えるために自然文に現れないトークンを使った方がいい、ということ


ViGGOの解説
- なるほど、かなり厳密なルールに従ってやりとりすることを期待しており、その厳密なルールを自然文で伝えると、GPT4でもうまくできない、という問題のようだ

ファインチューニングの有効性
> 以前のブログ記事で、私たちは「ファインチューニングは事実のためではなく、形のためにある」という考えについて述べた。
- [Fine tuning is for form, not facts | Anyscale](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts)

いくつかの重要な質問
- ベースモデルが学習過程でタスクの概念に遭遇しているか？
    - 遭遇してない新概念は、小規模なファインチューニングで獲得できる可能性は低い
- Fewshotsで改善するか？
    - 改善するなら、ファインチューニングでさらに改善する可能性が高い
    - > モデル内部のニューラルネットワークの重みにはるかに多くの例を組み込むことができるからです。


ViGGOはパターン認識を中心に展開され、言語と基本的な概念の基本的な把握が必要だが、複雑な論理的推論は要求されない。
- さらに重要なこと: 出力に必要な「事実」はすべて入力にすでに埋め込まれている
- なるほどね、そういうタイプのタスクだからViGGOは7BのファインチューニングでGPT4を超えられたわけか<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>

評価
- GPT4は「属性の順序を守れ」が下手で、それを成功しているかどうかの判定につけることで9割から5割まで下がってる
- ファインチューニングでは順序が守れている



Llama-2微調整モデルによるSQL生成

- なぜ微調整が有望なのか？
    - > このタスクは、SQLの「構造」を学習し、自然言語をこの構造に変換するLLMの能力に成功がかかっている
    - これも「出力の形」がキッチリとルールに従うことが大事なパターンということだな<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>

結果
- 7BのファインチューニングでGPT-4と70B-chatを上回った
    - チャットのために学習されていると返ってフォーマットに従わなくなるようだ<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>


小学生の算数推論（GSM8k）
- > このデータセットでの微調整の課題は、前の2つとは異なる。単に構造を学習するのとは対照的に、我々はLLMが数学の問題に対する推論能力をどれだけ向上させることができるかを見たかった。

- 自然文で回答が出される場合に正しく答えられているか検証が困難なのでGPT-3.5で切り出した
    - ファインチューニングするとすぐに正規表現で切り出せるような出力をするようになってAPI呼び出しのコストが削減できた


chat版はそもそも7Bや13Bにおいて性能が高い
- たぶん学習データに数学のやり取りが含まれてる
- 70B 8shotのベースラインにチャット版は全敗してる
- ファインチューニングすると性能は上がるし、トークンコストは下がる

8kデータポイントでは足りないと判断してさらに増やすアプローチをして、さらによくなったと言っている
- 頑張ってるなぁ<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>