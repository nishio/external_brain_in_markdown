---
title: "画像生成AI勉強会(2022年10月ダイジェスト)"
---

サイボウズラボ勉強会 2022-11-11
- 前回9/30の[[Stable Diffusion勉強会]]から1ヶ月の間に画像生成AI周りでは激しいストーリー展開があった
    - 今回はそれをダイジェストで振り返って、残りの時間で[[Imagic]]と[[Aesthetic Gradients]]の仕組みを説明する
- 10/3 小説作成AIのサービスを提供していたNovelAI社、有償の画像生成AI NovelAIDiffusionをリリース
    - アニメ絵特化で高クオリティに騒然
    - Stable Diffusionではできなかった任意アスペクト比の画像の学習・生成が可能
    - 日本語圏では学習元が無断転載サイトであるとして怒り出す人が発生
- 10/7 NovelAIDiffusionのソースコードとモデルが流出、Torrentで共有される
- 10/12 NovelAI、リリースから10日で生成された画像が3000万枚を突破とアナウンス
    - 大雑把にいって1日300万円売り上げが立つイメージ
- 10/17 中国語のNovelAIプロンプトマニュアル「元素法典」が話題に
    - 中国語圏でNovelAIの流出モデル利用がメジャーである傍証
- 10/18 Imagicが話題に
    - とても有益、ちゃんと使えるという話と、いまいち期待したように使えないという意見がある
    - 僕は後者なんだけど、これって「上手い使い方を理解してないだけ」の可能性がある
- 10/20 Stable Diffusion、1.4をリリースしたStability AI社ではなく、Runway社から1.5がリリースされる。Stability AIは一時削除申請をするが、後に取り下げる
- 10/21 Stability AI、(大慌てで？)新しいVAEをリリース、目や顔のデコードを改善するもの
- 10/22 日本語でNovelAI関連の情報を発信していた人の自宅に変な人が来て警察沙汰に
- 11/3「NovelAI Aspect Ratio Bucketing」がMITライセンスで公開

NovelAIDiffusionリリース
- 小説作成AIのサービスを提供していたNovelAI社、有償の画像生成AI NovelAIDiffusionをリリース
- Stable Diffusionではプロンプトが77トークンで打ち切られていたが、NovelAIDiffusionでは3倍の231トークン
- Stable Diffusionでは学習データが正方形にトリミングされていたがNovelAI社の工夫によって任意のアスペクト比での学習・生成が可能になった
    - 論文公開を目的とする大学研究室と違って営利企業のサービスなので詳細は非公開だった(後に公開された)
    - アスペクト比は構図に強く影響する
 NAI Curated

```
girl, blue eyes, blue long hair, blue cat ears, chibi
```

        - ![image](https://gyazo.com/ec303056563dd0308f6530af5549d053/thumb/1000)![image](https://gyazo.com/a8a40c57789dea0cd4e523c2ed84999c/thumb/1000)![image](https://gyazo.com/e34a2583abf1105d02ba614f08c2877d/thumb/1000)
    - 生成される絵の分布が著しく偏っている
        - プロンプト"black cat"で5枚生成して2枚が猫耳少女 [[日記2022-10-07]]
            - ![image](https://gyazo.com/487f8d241846f06d4a34770a344703db/thumb/1000)![image](https://gyazo.com/65e72a194351fed5c17fc59eb07d4961/thumb/1000)
            - 「とりあえずStable Diffusionとの比較の為にblack catって入れてみるかー」で1枚目が出てお茶を吹きそうになったw
        - 得意分野「アニメ調の女性」に対して圧倒的強さを示してSNSが騒然となった
            - 当日のTweetがここに記録されている [https://note.com/yamkaz/n/nbd9a028d625a](https://note.com/yamkaz/n/nbd9a028d625a)
            - ここに記録されているTweetがほとんど「アニメ調の女性」なのが特徴的
            - 多種多様な「絵」の分布のうちの狭い領域に対して特化してリソースを注ぎ込んだことにより、その領域においてユーザ価値が分水嶺を超えた
                - 他の領域に関しては表現力が下がるわけだが、伸ばした特徴が顧客に刺さったというわけだ
                - [[ブルーオーシャン戦略]]だね
- 学習に使われているデータセットに対して議論が巻き起こった
    - 学習にDanbooruという「有志が画像にタグ付けをして、タグから画像を探せるようにしているサービス」のデータを使っている
    - 賛否両論(というか少なくとも日本語SNSでは否定的意見が大きな声で発信された)
        - 否定的意見:
            - Danbooruは無断転載サイトであり、違法だ。
            - 違法なデータで学習したAIは悪だ、敵だ。
            - このAIは有償サービスだ、そこで得た利益は我々から盗んだものだ。
    - ところでDanbooru自体は元画像に対する出典を明記し、リンクを貼っているので、この「無断転載」に違法性があるのかどうかはなかなか難しいところ
        - ![image](https://gyazo.com/18161037994ef05c4645892aeac400f7/thumb/1000)![image](https://gyazo.com/f06398234bfe68c7bda296b4c332b7ed/thumb/1000)
            - Pixivから転載した旨が明記されている
        - [[フェアユース]]なのでは説 [フェアユース - Wikipedia](https://ja.wikipedia.org/wiki/フェアユース)
            - 複製物の使用が市場（潜在的な市場を含む）に悪影響を与えるか？
            - 元々無償で公開されてるものを出典明記して転載する行為が市場に悪影響を与えると主張するのは困難
        - Google検索で検索結果にGoogleのサーバに複製された画像キャッシュが表示されることとの関係
            - 画像が小さければ「これは検索結果のサムネイルです」になる
            - 画像が直リンクなら「複製はしていません」になる
            - 画像が大きいので意見がわかれる感じ
        - もちろんユーザ投稿コンテンツなので、中には違法にアップロードされているものもあるかもしれない
            - (例えばオンラインで公開していないデジタルコミックからの転載など)
            - でもそれに関しても[[デジタルミレニアム著作権法]](DMCA)に則って運用してればサービス運営主体は罪には問われないわけで:
                - ノーティスアンドテイクダウン手続 (DMCA通告)
                - > 著作権者の許可なく著作物が第三者によってウェブサイトに掲載されたと通知 (notice) を受けた場合、そのウェブサイトの運営者が速やかに削除 (takedown) すれば損害賠償などを免責される
            - 非公開コンテンツの転載被害にあった人からは「Danbooruが悪い」というように嫌われているだろうけども、法的にはDanbooruは悪くない、被害者の側にノーティスの責任がある
        - 10/5 [イラスト自動生成AI「NovelAI」について学習元となったDanbooru公式が声明を発表 - GIGAZINE](https://gigazine.net/news/20221005-novelai-danbooru/)
            - ざっくり「AIのことはNovelAIに言え、我々は関係ない。著作権者である証拠があれば削除には応じる」
            - DMCA的には無断転載されていると主張する側に立証責任があるから、そういうだろうね
    - 世界では「Danbooruを使うことに何か問題が？」という感じ
        - > [@MutedGrass](https://twitter.com/MutedGrass/status/1576861235458424833?ref_src=twsrc^tfw|twcamp^tweetembed|twterm^1576861235458424833|twgr^669f7044c21e6b7686422c72bc25ca959dc4862c|twcon^s1_&ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fnbd9a028d625a):
        - > ・StableDiffusion…LAION 5B にDanbooruの画像URLがある
        - > ・WaifuDiffusion…Danbooru 2021 データセット使用を明言
        - > ・NovelAI…Danbooru利用を明言。
        - > ・ミッドジャーニー…WaifuLabsとコラボしてSafebooru由来のデータを使う(予定)
        - > つまりみんなDanbooru使ってるやん！ となります
        - つまりNovelAIがDanbooruを使っていることに対する日本語圏の反応は[[集団極性化]]現象
            - 反対派が大きな声で叫んだのでニュートラル〜賛成派は被害を恐れて黙った
            - 「試しているけど情報発信してない」という意見を複数チャンネルで聞いた
                - [[新技術とパブリケーションバイアス]]
            - 「論理的に正しい意見でも、おかしな人に絡まれることはあるから控えた方がいいのでは」と僕に忠告してくれた人もいる
        - 10/22 [家凸に関して｜852話｜note](https://note.com/852wa/n/nee6da6dd577b)
            - 積極的に情報発信していた人の自宅に変な人が来た事例
        - 10/29 [AI絵師始めたけど数字エグい](https://anond.hatelabo.jp/20221029165248)
            - > twitter上では、「AIタグが付いてる絵は非表示にしている。やはり絵は人間が描いてこそ」という声も散見されるが、それが単なる「声の大きい少数派の意見」でしかなかったことが、pixivでの自アカの数字を見ていてまざまざと感じた事実だ。
        - 10/31 [pixiv お知らせ - AI生成作品の取り扱いに関する機能をリリースしました](https://www.pixiv.net/info.php?id=8728)
            - Pixivは「AI生成作品を排除せず、ランキングをわけて棲み分けさせる」という着地をした
            - 一方で11/10に確認した時点でDanbooruはAI作品の投稿を禁止している

NovelAI流出
- 10/7 NovelAIDiffusionのソースコードとモデルが流出、Torrentで共有される
    - リリースからわずか4日でw
- 10/12 NovelAI、リリースから10日で生成された画像が3000万枚を突破とアナウンス
    - ![image](https://gyazo.com/d969383120e2b7534cfa13e28d8e4fda/thumb/1000)
    - ![image](https://gyazo.com/009042f555c1f5f34189e86c14faade1/thumb/1000)
    - プリセットのサイズの中で一番小さい512x512で、デフォルトパラメータで4枚生産したら20anlasなので11ドルで2000枚くらい
        - (この後デフォルトパラメータが変わって16anlasになった)
        - 高解像度化とかにも使われてるだろうから大雑把に言えば1枚1円ぐらい
        - 大雑把にいって1日300万円売り上げが立つイメージ
- 10/17 中国語のNovelAIプロンプトマニュアル「元素法典」が話題に
    - ![image](https://gyazo.com/f2732d53db16958208ab0c02fe9369cf/thumb/1000)[docs](https://docs.qq.com/doc/DWHl3am5Zb05QbGVs)
    - > ![image](https://gyazo.com/d6123280b089eedc35c54fa78baf0c58/thumb/1000)
    - > あっさりできた [[日記2022-10-17]]
    - ![image](https://gyazo.com/7ea88e2f341de202cf6061ce045bb6a3/thumb/1000)
        - トークンのベクトル強調に使われているこの丸括弧、NovelAIのサービスでは機能しない
            - [[NovelAIで丸括弧を使っても無意味]]
            - 丸括弧はローカルでStable Diffusionを動かす際のデファクトスタンダードなAUTOMATIC1111/stable-diffusion-webuiの機能
            - つまりこれ、中国語圏ではNovelAIのサービスではなく、ローカルで流出モデルを使うのがメジャーである傍証
            - 流出モデルの使用、日本国内では「違法なのでやめましょう」的なことを言う人もいるけど、何の法に触れるんだろう？よくわからない
                - 日本の法律だと不正競争防止法2条1項5号かな？
                - > その営業秘密について営業秘密不正取得行為が介在したことを知って、若しくは重大な過失により知らないで営業秘密を取得し、又はその取得した営業秘密を使用し、若しくは開示する行為
                - NovelAIはデラウェアだったと思うけど、たぶん似たような法律があるんじゃないかな
                - まああったとしても中国のユーザを訴えるのは難しそうだ
- 流出がなければ元素法典は生まれなかっただろう。
    - NovelAI社にとって流出が悪いことだったのかどうかは、もう少し時が経たないとわからないかも

Imagic
- 10/18 Imagicが話題に
- > [@AbermanKfir](https://twitter.com/AbermanKfir/status/1582218210547757056?ref_src=twsrc^tfw|twcamp^tweetembed|twterm^1582218210547757056|twgr^93858c6e839b7b851b90c90aa0cade388dbeadde|twcon^s1_c10&ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn55c3ac365f34): The combination of #dreambooth and embedding optimization paves the way to new image editing capabilities. Love it. Congrats on this nice work!
    - [[DreamBooth]] + [[embedding optimization]]
- ![image](https://gyazo.com/c4b331f315d8d71419e2fb58ada3a5c7/thumb/1000)
- とても有益、ちゃんと使えるという話と、いまいち期待したように使えないという意見がある
    - 僕は後者なんだけど、これって「上手い使い方を理解してないだけ」の可能性がある
    - ![image](https://gyazo.com/905cdfcbae8f2199b00fbb470fd7db67/thumb/1000) + "a woman wearing black suit" = ![image](https://gyazo.com/a0f9ca631a50c9c47882cdb6ac64cb05/thumb/1000)
        - [Imagicを理解する - ほげほげ](https://birdmanikioishota.blog.fc2.com/blog-entry-12.html)
        - 注目するところは「顔が、同一人物と言われても違和感がない程度に保存されてること」
    - > [@npaka123](https://twitter.com/npaka123/status/1582696004414939137?ref_src=twsrc^tfw|twcamp^tweetembed|twterm^1582696004414939137|twgr^80908a8905a2b157f0902fc3a878d9d3d3b5735e|twcon^s1_c10&ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn7a7394323358): うちの猫を Imagic Stable Diffusion で寝かせようとしたら(a cat is sleeping)、猫は変わらず、寝室っぽいとこに移動した
    - > ![image](https://pbs.twimg.com/media/FfbbX54VsAAUp2Z.jpg)![image](https://pbs.twimg.com/media/FfbbZARVUA0ydqG.png)
        - 猫が保存されてる
    - [[Imagic 2022-10-31]]
        - ![image](https://gyazo.com/6b7beb6c41765ff93c1bdede39f5d14a/thumb/1000)![image](https://gyazo.com/ece7c6d8f55c16a421c86b70afdf5204/thumb/1000)
        - プロンプトにwith flowerをつけた
        - デフォルトではstrengthが0.9だが、それでは全然変化しなかったので増やしていくと花がついた
    - [/villagepump/2022/10/30#635eaefae2dacc0000f46cc2](https://scrapbox.io/villagepump/2022/10/30#635eaefae2dacc0000f46cc2)
        - ![image](https://gyazo.com/bdd1d9b05d5d826c4b5b623fdd88fb70/thumb/1000)
    - 実写だと機能しやすいという意見 [/villagepump/2022/11/01#6360b5eee2dacc0000329928](https://scrapbox.io/villagepump/2022/11/01#6360b5eee2dacc0000329928)
        - まあ確かにNovelAIのモデルで生成したものをStable DiffusionのモデルでImagicしてまともに動いてることの方がすごいのか
        - > <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>img2imgに比べて2桁くらい時間コスト高い割に、元絵をそれほど維持してくれるわけでもない
            - モデル違うので、維持しない方が普通
- 原理などの話を今回の発表の最後に加筆した

Stable Diffusion 1.5
- 10/20 Stable Diffusion、1.4をリリースしたStability AI社ではなく、Runway社から1.5がリリースされる
    - Stability AIは一時削除申請をするが、後に取り下げる
    - Stability AI側が共同研究の成果物の権利範囲をきちんと把握してなかったミスだろうな
        - 独占的権利があると思い込んでいたが、実はなかった、的なやつ
    - Runwayの側としては知名度向上のチャンスなのでリリースするのは合理的
        - この件でRunwayを知ったり意識したりし始めた人も多いと思う、僕もそう
    - 考察
        - [https://note.com/yamkaz/n/n165fa3922570](https://note.com/yamkaz/n/n165fa3922570)
        - Stability AI側はNSFW対策を進めたいが、対策してないモデルもリリースしたいのでRunwayがリリースしたのでは説
        - 無理があると思う
            - Runwayだって私企業な訳なのでリスクを引き受けるインセンティブなくない？
            - その目的だったらNovelAIと同じように「匿名のハッカーの攻撃で流出した」というポーズで流出させればいいだけでは。
- 10/21 Stability AI、(大慌てで？)新しいVAEをリリース、目や顔のデコードを改善するもの
    - 1.6をリリースできる状態にないが、Runwayが最新である状態が長く続くのも困るので、リリースできるところを取り急ぎリリースしとけ的なことだと解釈してる
    - Runwayからの1.5のモデルとStability AIからのVAEを手元で組み合わせて「顔の表現がすごく良くなった！」と言ってる人はいる
    - が個人的には「dependency hellが始まりそうだなー」という気持ちになって距離を置いてる
- Runway: AI Magic Tool
    - 動画編集を中心に色々な便利機能を詰め合わせたサービスを提供している
    - Infinite Image
        - いわゆるoutpainting
        - ![image](https://gyazo.com/e2ba3a5007a13db2ed0b672d38e628be/thumb/1000)![image](https://gyazo.com/2f924b10840f6848a8abba45616879c5/thumb/1000)
        - 遠くから見れば合成したとわからない？
        - 合成したい範囲を指定して
            - ![image](https://gyazo.com/afd13ca995fbfe6726ee3e8be4d36a03/thumb/1000)
        - 生成ボタンを押すと4枚作られて選ぶことができる
            - ![image](https://gyazo.com/ff857109afe0720fb5009cd51f811f71/thumb/1000)![image](https://gyazo.com/24c6ac45328412eb8f770c16c801ea99/thumb/1000)![image](https://gyazo.com/27b221e146cc92face56920c611b8243/thumb/1000)![image](https://gyazo.com/8b9aec27e53f5feed3f48a488f19017f/thumb/1000)
        - アニメ調はあまり得意ではなさそう
            - ![image](https://gyazo.com/6b49334374d1adfffa61f036768f12ca/thumb/1000)→![image](https://gyazo.com/65af1d0f78bebbb48de566a423ceb535/thumb/1000)
            - NovelAI img2img Noise 0 Strength 0.5してみた
            - outpaintingでは元々あった画像(表情とか)は変化していない
            - img2imgでは大まかには同じだが細部は変わってしまう
    - Erase and Replace
        - いわゆるinpainting
        - おそらくノイズでフィルするので[[Runwayのinpaint上手くいかなめ]]
            - 消した範囲に謎のものを出現させがち
    - その他、動画の物体トラッキングや音声のノイズ除去などが詰め合わされている

NovelAIDiffusionを支える技術
- 10/11 [NovelAI Improvements on Stable Diffusion | by NovelAI | Oct, 2022 | Medium](https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac)
- 10/22 [The Magic behind NovelAIDiffusion | by NovelAI | Oct, 2022 | Medium](https://blog.novelai.net/the-magic-behind-novelaidiffusion-b4797e0d27b2)
- 11/3「[NovelAI Aspect Ratio Bucketing](https://github.com/NovelAI/novelai-aspect-ratio-bucketing)」がMITライセンスで公開
- 10/11に技術的に尖った話を書いたんだが世の中が根本的に画像生成AIの仕組みを理解してなくて「データベースの画像をつぎはぎする」とかでたらめなことばかり言うから「違うからね！」と基本的な話をやったのが10/22の解説
- NovelAIDiffusionを支える魔法(10/22)
    - オリジナルのStable Diffusionはおよそ150TBのLAIONデータセットで学習されている
    - 530万件、6TBのデータセットでファインチューニングしている
        - このデータセットには詳細なテキストタグがついている
        - (これがたぶんDanbooru由来)
    - モデル自体は1.6GBで、外部データを参照せずに画像を生成できる
        - 学習中にサイズは変わらない(=から画像を覚えてるのではないよ！と言いたいわけ)
    - モデルの学習には3ヶ月掛けている
        - 3ヶ月学習の処理を走り続けさせたと言う意味ではなく、途中経過を人間が見て問題点修正するための開発をして〜を繰り返してる
        - ゴールが論文を書くことではなく、いいモデルを作ってサービス展開で金を稼ぐことなので、途中で人力試行錯誤を挟んでもいいってわけ
    - モデルの学習はNVSwitchを介して連結された8枚のA100 80GB SXM4カードと、1TBのRAMを搭載した計算ノードを使用した
- NovelAIによるStable Diffusionの改善(10/11)
    - CLIPのpenultimate layerの隠れ状態を利用する
        - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>penultimate layerは「finalレイヤーの一つ前のレイヤー」
        - Stable DiffusionはCLIPのtransformerベースのテキストエンコーダーの最終層の隠れ状態をclassifier free guidanceのガイダンスに使う仕組み
        - Imagen (Saharia et al., 2022) では、最終層の隠れ状態の代わりに、penultimate layerの隠れ状態をガイダンスに使用する。
        - EleutherAI Discordでの議論
            - CLIPの最終レイヤーは類似度検索に使うための小さなベクトルへ圧縮する準備をする
            - そのために値が急激に変化する
            - だからその一つ手前のレイヤーを使った方がCFGの目的には良いかもしれない
        - 実験結果
            - Stable Diffusionで最終の手前のレイヤーからの情報を使っても、多少精度は落ちるが、プロンプトにマッチした画像を生成することができた
                - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>これは自明じゃない、なぜならImagenはLDMではないから
            - 最終レイヤーの値を使った場合、色漏れが起きやすくなる
                - 例えば「初音ミク、赤いドレス」で、ドレスの赤い色がミクの目や髪の色に漏れてしまう現象
    - アスペクト比バケット
        - 既存の画像生成モデルには不自然な切り出しの画像をつくってしまう問題があった
            - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>人物画のくびがないとかのこと
        - これらのモデルが正方形画像を生成するように学習されていることが問題
            - ほとんどの学習元データは正方形ではない
            - バッチで処理する時に同じサイズの正方形に揃っていることが好ましいので元データの中央だけ取り出して学習させる
            - そうすると、たとえば「王冠を被った騎士」の絵は頭と足が切り落とされて肝心の王冠がなくなる
            - ![image](https://gyazo.com/13aa293442bfe496be831c2c15fd1e69/thumb/1000)
            - これによって人間が頭と足のない状態で生成されたり、剣が柄と先端のない状態で生成されたりする
            - 小説生成AIサービスの付属サービスを作ろうとしていたので、これでは全然ダメだった
            - また「王冠を被った騎士」の王冠がない状態での学習はテキストと内容が不一致でよくない
        - センタークロップの代わりにランダムクロップにするのを試したがわずかに改善するだけだった
        - Stable Diffusionを色々な解像度で学習させることは簡単だが、画像サイズがバラバラだとバッチにまとめられないのでミニバッチ正則化ができず、学習が不安定になる
        - そこでバッチ内の画像サイズは同じで、バッチごとの画像サイズは異なるようなバッチ作成を可能にする実装をした
            - それがアスペクト比バケッティング
        - アルゴリズムをざっくり言えば色々なアスペクト比のバケツを用意しておいて、一番近いアスペクト比のところに画像を入れる
            - 多少のズレは構わないよねということ
            - 少しズレる分はランダムクロップする
                - ほとんどのケースで32ピクセル未満の削除で済む
    - トークン数を3倍に拡張
        - StableDiffusionのトークン数は最大77
            - 75にBOSとEOSをつけたもの
        - これはCLIPの制約
        - そこでプロンプトを75、150、225のいずれかに切り上げ、75トークンごとに分割し、個別にCLIPに通して、ベクトルを結合する
    - ハイパーネットワーク
        - 2016年にHaらが提案した同名の手法とはまったく関係ない
            - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>知らずに名前をつけてかぶったんだな
        - 大きなネットワークの中の複数のポイントから小さなニューラルネットを使って隠れ状態の修正に使うテクニック
        - プロンプトチューニングよりも大きな(明確な)影響を与えられるし、モジュールとしてつけ外しできる
            - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>これはエンドユーザが部品として認識してつけ外しすることができるスイッチを提供できることがサービス提供上のメリットであるということ
            - 小説生成AIをユーザに提供してきた経験から、ユーザに機能切り替えスイッチを提供することに関してユーザが理解できること(とおそらくユーザの満足度向上につながること)がわかっていた
            - ![image](https://gyazo.com/4ba1538c98f240966cbc4120215db499/thumb/1000)
                - [https://novelai.net/](https://novelai.net/)
        - パフォーマンスが重要
            - 複雑なアーキテクチャだと精度が上がるが、それによる速度低下は本番環境で(実際にAIをエンドユーザが触るサービスとしたときに)大きな問題になる
        - 当初は(小説生成AIですでに試していたのと同じように)埋め込みの学習を試みた
            - これはTexual Inversionに相当するもの
            - しかしモデルが十分に汎化できなかった
        - そこでハイパーネットを応用することにした
            - 色々試した結果、クロスアテンション層のKとVの部分だけに触ることにした
            - U-netの他の部分には触らない
            - 浅いアテンションレイヤーが過学習するから学習中にペナルティを課す
            - この方法でファインチューニングと同様かそれ以上の性能を発揮できた
                - 対象概念に対するデータが限られているときに特にファインチューニングよりも良い
                - オリジナルのモデルが保持されたまま、ハイパーネットが潜在空間のデータにマッチするスパースな領域を見つけられるからだと思う
                - 同じデータでファインチューニングしようとすると、少ないトレーニング例に合わせようとして汎化性能が落ちてしまう
                    - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>たぶんモデル全体のファインチューニングでは自由度が高すぎて、全体の重みで少しずつ学習データを表現しようとしてしまうのだろう
                    - アテンションの調整だけに限定したことで「条件ベクトルによってデノイズする仕組み」はたくさんのデータで学習したまともな状態のまま温存しつつ、それに入力するベクトルが単なるtransformerが作るものよりもドラスティックに変化するようになる、ということかと

[[Imagic]]
- 1枚の画像とテキストプロンプトを元に新しい画像を生成する仕組み
- 入力するものはStableDiffusionの img2imgに似ているが、img2imgではできないような画素の大局的な変更ができるのが特徴
- ![image](https://gyazo.com/ded80c6786c8a03b034121c7e7c793ff/thumb/1000)[PDF](https://arxiv.org/pdf/2210.09276.pdf)
- どういう仕組みか？
- ![image](https://gyazo.com/62f14b20e57c5aea68ef4c72e0269af7/thumb/1000)

    - StableDiffusionは大まかにいえば「テキストを入力として画像を出力する、テキストと画像のペアで学習する」
        - だが、箱を開けてみると中にはフリーズされた[[CLIP]]が入っていて
        - テキストは埋め込みベクトルの形になってから[[LDM]]に渡されている
    - SDの学習とは埋め込みベクトルeと出力画像xを固定して、ロスLが最小になるようにLDMのモデルパラメータθを更新する作業
        - ![image](https://gyazo.com/91545820622700ae4ba48769e2685776/thumb/1000)
        - Imagicは3ステップに分かれている
            - 1: まず画像とモデルパラメータを固定して埋め込みベクトルを最適化
                - ここのロスはStableDiffusionと同じ、[[DDPM]]の普通の定義
            - 2: それからその埋め込みベクトルにを固定してモデルパラメータを最適化
                - (高周波成分を保持するための補助ネットワークが付け加えられている)
            - 3: eとeoptを線形補間して新しいLDMの入力にして画像を出力する
- 図解
- ステップ0
    - ![image](https://gyazo.com/1328e4f076205f6937e2f97086c19bc5/thumb/1000)
    - ケーキの写真と「ピスタチオケーキ」というプロンプトが与えられている
    - もちろん「ピスタチオケーキ」というプロンプトから作られる画像は与えた画像とは全然別物
- ステップ1
    - ![image](https://gyazo.com/2f6bdf14c72b0623d4f45bd9ac89a664/thumb/1000)
    - 出力される画像が入力画像xに近くなるように埋め込みベクトルeを更新する
    - この図の画像は似過ぎだと思う
        - (論文にこのときの画像が明確には示されていない、だいたいこんな感じとは書いているが後述の補助モデルの影響も含んでそうに見える)
- ステップ2
    - ![image](https://gyazo.com/e6794fbaf4198641b9d9acf04de66f94/thumb/1000)
    - 補助モデルを組み合わせてeoptから生成した画像と入力画像xとの差が小さくなるようにモデルパラメータθを更新
    - このとき補助モデル部分がLDMでは表現できない細部を学習して吸収するので、ほぼ同じ画像になる
    - 補助モデルは高周波成分を保持するために付けられている
        - 「細部がよく保存されてる！」と感じるのは、このネットワークがLDMでは保持されない高周波成分を保存してるから
        - LDMは8×8の画素を1ピクセルに潰してしまっているので画像で与えられた情報の高周波成分は失われる
        - 細部はVAEのデコーダが復元してるので、それでは画像で与えた個人の顔を保持することはできない。補助モデルが差分を吸収する
- ステップ3
    - ![image](https://gyazo.com/c816daacb1972684d04fc7e8d0bf1cdc/thumb/1000)
    - この新しいモデルが生成する、1次元の空間の中のどこかに「得たいものに割と近いものがある」と主張している
        - ここでは「小さい空間なら平坦とみなして良いだろう」という仮定が入っている
    - 混合係数0.7くらいが良さげとの主張
        - まあこれは写真でやった場合の話で、僕がNovelAIで使ったアニメ絵で実験したら0.9でもほとんど元画像と同一(背景色が違うくらい)だった
- 考察
    - img2imgと違ってダイナミックな変化が起きるな？と思っていたがそれはそう
        - 入力はimg2imgと同じだが、img2imgと違って与えた画像を後で画像を生成するときの初期値に使っていない
            - 生成プロセスは補助モデル付きのtxt2img
        - img2imgでは与えた画像をダウンスケールして(VAE encodeして)それを初期値に絵を描いてる
            - 目の悪い人が元絵を参考にしつつ絵を描くみたいなもの
            - なので赤い服の絵を渡して青くしろというのは無茶
        - Imagicは赤い服の絵を渡して「これが青い服の絵です」と言う
            - 埋め込みベクトルの更新で「青い」という単語の意味が『赤い』に動かされる
            - その上で与えた「赤い服の絵」が再現されるようにLDMと補助モデルを更新する
            - そして「青い」の単語の意味を『赤い』から『青い』に戻すと「青い服の絵」が生成される
    - 顔などの高周波成分が保存されているのは、普通にSDした場合に消し飛ぶような顔の詳細を「補助モデル」が吸収しているから
    - なぜアニメ絵で0.9でもほとんど元画像と同一(背景色が違うくらい)という現象が起きたか
        - 写真でも同じように「変わってほしい対象物ではなく背景が変わった」というケースがある
            - > [@npaka123](https://twitter.com/npaka123/status/1582696004414939137?ref_src=twsrc^tfw|twcamp^tweetembed|twterm^1582696004414939137|twgr^80908a8905a2b157f0902fc3a878d9d3d3b5735e|twcon^s1_c10&ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn7a7394323358): うちの猫を Imagic Stable Diffusion で寝かせようとしたら(a cat is sleeping)、猫は変わらず、寝室っぽいとこに移動した
            - > ![image](https://pbs.twimg.com/media/FfbbX54VsAAUp2Z.jpg)![image](https://pbs.twimg.com/media/FfbbZARVUA0ydqG.png)
        - 補助モデルが対象物のほとんどの情報を吸収したんじゃないかな
            - 顔同様の「LDMの外で保持すべき情報」とみなされた
            - このアルゴリズムは何が変化させたい対象物であるかを判断してない
            - 画面の大部分をしめて、SDがプロンプトから高確率で出せないような対象物は「SDで出せないから補助モデルで吸収しよ〜」となる
        - 混合比率ηは後から変えて試せる
            - ここはプロンプトのベクトル混合にすぎないので無視できるくらい軽い
            - 内分だけでなく外分もできる

Aesthetic Gradient
- /ɛsˈθɛt.ɪk ˈɡɹeɪdiənt/
- ![image](https://gyazo.com/45c6ce5f020171485b09f1355715ece5/thumb/1000)[PDF](https://arxiv.org/pdf/2209.12330.pdf)
- ユーザの美的感覚を抽出してパーソナライズに使おうという研究
- 仕組み
    - テキストプロンプトをCLIPのテキスト埋め込みでベクトルにしたものc
        - StableDiffusionのデフォルトなら768次元ベクトルになる
    - そのプロンプトに対応しているユーザの好みの画像N枚のCLIPの画像埋め込みでベクトルにしたものの平均e
    - ベクトルを正規化しておけば内積が類似度としてみなせる
        - なのでeだけ取っておけばCLIPのテキスト埋め込み部の重みを勾配降下法で最適化できる
        - 学習率1e-4で20ステップくらいでよい
- 考察
    - CLIPで各トークンがどのようなベクトルに埋め込まれるのかを微調整する手法
    - Textual Inversionでは意味のないトークンにがっつり意味を与えていたが、この手法は既に意味を持ってるトークンのベクトルをユーザの好みの方向に少し寄せるだけ、あまりかけ離れたものでは(学習回数も少ないので)上手くいかないと思う
        - そのかわり学習はものすごく軽い
    - TIと違って本質的に複数単語OKな手法であるところも長所
        - 長めのプロンプトから2N枚画像を作って、そのうちの好みなN枚でAGを作るとかすると良いのかも
        - 例えば[[Stable Diffusion埋め込みテンソル編集]]で行った実験では人間がcatとkittenを混ぜて単語に対応してないベクトルを作った
            - NovelAIは標準機能でこの機能を持っている、混ぜる比率は人間が手で決める
            - Aesthetic Gradientはcatやkittenで作った画像から好きなものだけ選んで学習させることで自動的に「適度に混ざったベクトル」を作るものと言える
    - 画像はCLIPでベクトルに変換してから使うのでサイズ調整とかが必要ないのも長所か
    - 目的関数がCLIPのそれなのでCLIPのタスクである画像と文章の類似度判定に有用でない特徴は無視されそうな気がする
        - =文章に現れないような特徴が無視されそう(たかだか768次元しかないし)
        - 一方でベクトル調整で得たいのは「文章でうまく指示できないような好み」だと思うので、どうかなー
        - 「文章で表現することは可能だが人間がそれを上手く表現できてない」系に有用かな

最後に
- やっぱDreamBoothが本命中の本命な気がする
    - 高コストなので「もっと簡単な手法を作った！」的な論文が色々出てるけどどれもイマイチに感じる
- 二番手がHypernetworkなんだけど、これは論文にもなっておらず詳しい情報も非開示で「NovelAI社がそれを使ってNovelAIDiffusionを使った」「ソースコードが流出した！」な状態なので情報が錯綜している…
    - これもアテンションをいじる方法なので、元々Stable Diffusionが描けるものしか描けない、Danbooruの大量のタグで学習したのでコントロール効きやすくなっただけ
    - ![image](https://gyazo.com/1265c59ef89df55dbbf0517191fd4946/thumb/1000)
        - こういうイメージ
        - 全体の表現能力(黒丸の数)自体は変わっていない
        - 特定の画風領域に黒丸を集中させた
        - それによってその領域における点の密度が上がった
            - その領域だけに注目した場合、表現能力が上がったように見える
            - [[認知の解像度]]的な話
    - HypernetworkはLDM本体のモデルよりだいぶ小さくて、モジュールとしてオンオフできるのでアニメ絵に関しては「人物用」「背景用」とかに細分化していくかも
