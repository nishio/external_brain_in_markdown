---
title: "劣微分"
---

[[FOBOS]]
[劣微分を用いた最適化手法について(4) | Preferred Research](https://research.preferred.jp/2010/12/subgradient-optimization-4/)

> 前回の最後で、[[L1正則化]]はパラメーターの非ゼロ要素を減らす効果が強いが、wi = 0の位置で微分不可能なので[[SGD]]ではそもそも計算できない、という話を書きました。……でも、よく考えてみると、微分の値として-1から1の間で適当な値を取ってもいいんじゃないかな…と思いませんか？ その疑問に答えてくれるのが[[劣微分]]です。
[劣微分を用いた最適化手法について(3) | Preferred Research](https://research.preferred.jp/2010/12/subgradient-optimization-3/)

> [[L2正則化]]の場合、wiの値が0に近づけばペナルティはほぼ0になりますが、L1正則化の場合、ペナルティが0になるのは値が完全に0であるときのみです。そのため、L1の方がパラメータベクトルの非0の要素を減らす力が強く働きます。こう書くとL1正則化の方が良さそうですが、0のところで[[微分が不可能]]なため、一般的な数値最適化の手法を使うのが難しくなります。それで、L1正則化を実現するためのさまざまな研究が生まれました。
- [劣微分を用いた最適化手法について(2) | Preferred Research](https://research.preferred.jp/2010/11/subgradient-optimization-2/)

- [劣微分を用いた最適化手法について(1) | Preferred Research](https://research.preferred.jp/2010/11/subgradient-optimization-1/)

[[Cumurative Penalty]]
- [劣微分を用いた最適化手法について(完) | Preferred Research](https://research.preferred.jp/2011/02/subgradient-optimization-5/)

L1とL2を混ぜた線形回帰を[[ElasticNet]]という
[http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)
