---
title: "思考の結節点2025-06-06"
---

- [[広聴AIと濃いクラスタ抽出]]
- [[Talk to the Cityのクラスタリング]]
- [SNS研究者から見た都知事選〜ここがすごいよTTTC〜 #安野たかひろ｜M](https://note.com/m_datasci/n/n3a5f4b9cdee5?magazine_key=m768f1f8d68ef)

[dd2030 slack](https://dd2030.slack.com/archives/C08F7JZPD63/p1749041733485549) 2025-06-04
中山心太（tokoroten）
- 西尾さんと雑談して、都知事選の時のtttc運用のノウハウ（と現在のチームみらいの運用ノウハウ）を掘り起こす会を開く必要がある気がしてきた
- tttcと広聴AIの大規模データの取扱経験があって、データサイエンスやエンジニアの言葉で課題意識や運用ノウハウを話せる人が足りない

NISHIO Hirokazu
- ちなみにそれ nasukaさん7割、僕3割くらい
- [[シン東京2050ブロードリスニング]]は安野チーム側ではなくGovTechTokyo側にガッツリやってた人がいて安野チーム側は支援やアドバイスをしてるだけで実データの分析はしてないんだよね
- あ、違うか、都知事選の時の話か

中山心太（tokoroten）
- そう、都知事選の時のノウハウを吸い上げたい
- [東京都知事選2024におけるTalk to the Cityの活用ノウハウ｜NISHIO Hirokazu](https://note.com/nishiohirokazu/n/n0661204bda5b)
- これの三段くらい深い話がほしい

NISHIO Hirokazu
- 他になんかあったかなぁ、
- プレビューサーバがついてないと非エンジニアとの共有が辛いよね、は広聴AIで解決したし、
- UIがデフォルト英語だと日本人が離脱するよね、も広聴AIで解決した
- 当初YouTubeコメントとTwitterとを試したけど、YouTubeコメントは時間が限られてるのでみんな言葉足らずの投稿をするからこの手の分析にあまり向いてない(ノイズ多すぎになる)
- Twitterの方がマシだが、お金がかなりかかるし、結局「普段政治的発言をしてない人はTwitterに書くことを忌避する」とか
- 「誰も取り残さない」のために結局Google Formを併設したとか

中山心太（tokoroten）
- インサイトを得ようとするときにどう使うのかとか。
- ダミーデータとか、他人事のそれっぽいデータしか食わせてないので、データや分析結果に対してまともに向き合ってないので。
- データや分析結果に真面目に向き合ったときに、どう見るのか、何を見たいのか、何を得たいのか。

NISHIO Hirokazu
- インサイトねー
- データ分析のリテラシーがない一般人に見せるのに適した設定と、データ分析をしてインサイトを得たい時の設定は明らかに違うよね
- まあ「まずはざっくり、それから細かく」という原則でいいんだけど
- 一般人は7クラスタくらいでもう多いと思うし、7クラスタの説明を読まずにラベルだけで判断しがちだよね
- インサイトを引き出そうとしたらまあ30クラスタに割ったりすることになるよね
- 30クラスタでも構造を把握する上ではだいぶ不足で、クラスタを掘り下げたいニーズが出てくる
- クラスタ分けしてからデータをクラスタごとに分けてさらにTTTCをする手はあるが、とても面倒なので広聴AIでは階層を導入した


2025-06-06
- Shingo OHKI
    - たまたまヒットしたのですが、広聴AIの開発の経緯として重要そうな西尾さんのメモを見つけました
    - [[広聴AIと濃いクラスタ抽出]]
- NISHIO Hirokazu
    - ありがたい、書いたことをすっかり忘れてて自分ですぐに出せなかった
    - 先日 @中山心太 にも別途説明してた話
- 中山心太（tokoroten）
    - あー、濃いクラスタって、HDBSCANの疑似実装だったのか、ようやく意味が分かった

<img src='https://scrapbox.io/api/pages/nishio/o3/icon' alt='o3.icon' height="19.5"/><img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>データサイエンティスト向けダイジェスト解説
- クラスタリング手法の選択ロジック
    - Talk to the Cityが使っているBERTopic は内部で UMAP→HDBSCAN を動かしますが、Talk to the Cityはさらに Spectral Clustering でも分割していて一貫性が崩れる可能性があります。(see [[Talk to the Cityのクラスタリング]])
    - 改善策は (1) HDBSCAN の出力をそのまま使い閾値で微調整、または (2) BERTopic をキーフレーズ抽出器と割り切ってクラスタリングは外部に一本化、のどちらか。
    - 広聴AIは階層クラスタリングに一本化しています

- UMAP 前後のクラスタリング比較
    - 高次元空間でクラスタリングすれば距離は保たれますが計算が重い、可視化が困難
    - UMAP で 2D に射影してから分割すると処理は速く GUI で映える一方、局所距離が歪むので「近く見えて遠い／遠く見えて同じクラスタ」が発生する可能性があります。

- Talk to the Cityが用いるSpectral Clustering の“途切れクラスタ”問題
    - ラプラシアン固有空間で切るため、2D 座標そのものより k-近傍グラフの連結性 を優先します。
    - そのため点列が地理的に分断されても同ラベルになり得る――可視化時には注釈や補助線で説明が必要です
    - この現象はよく質問され、説明コストが高いので、広聴AIではこの現象が起こららないアルゴリズムを採用しました(Spectral Clusteringをやめて階層クラスタリングに変更)

- 「濃いクラスタ抽出」アルゴリズムの変遷(see [[広聴AIと濃いクラスタ抽出]])
    - 元々は 高次元 HDBSCAN で密集領域を検出。
    - 実運用ではコスト削減のため UMAP→細分化 k-Means→密度フィルタ で近似。
    - 元々のHDBSCANでは1万件のデータで10分程度追加でかかります。
    - 西尾は「将来は高速な分析をした後で、プラグインとして追加分析を実行できる仕組みにし、そこで高次元 HDBSCAN を復帰させるといいのでは」という構想。
    - 追加分析プラグイン構想
        - コア可視化パイプラインと切り離し、破壊的な新手法を “追加分析” として注入可能に。
