
目的
- ナイーブベイズについて簡単に説明する
- GaussianNBとBernoulliNBの違いを説明する

スパムフィルタで説明する
やりたいこと:
- S: メールがスパムかどうか
    - を判定する二値分類
簡単にするため使える情報は次の2つだけとする:
- A: 「メールに単語Aが含まれるかどうか」
- B: 「メールに単語Bが含まれるかどうか」

やりたいこと:
「AとBの情報から、S=0かS=1かを判定する」
確率の言葉で表現すると
「AとBが与えられたときの、Sの条件付き確率を求める」
- $P(S|A,B)$
これはたとえば「Aが出現して、Bが出現してないときのスパムである確率」$P(S=1|A=1,B=0)$ を略記している

これをどうやって求めるか？
ベイズの定理を使うと以下の等式が成り立つ。
- $P(S|A,B) = \frac{P(S)P(A,B|S)}{P(A,B)}$
つまり、過去のメールから
- スパムである確率 $P(S)$
- スパムであるときにAとBがたとえばA=1, B=0になる確率 $P(A, B|S)$
- AとBがたとえばA=1, B=0になる確率 $P(A, B)$
を調べればいい。単に発生回数を数えて全体で割るだけ。

ここまでただのベイズ。
ここまでは単語はAとBの2種類だけだった。
一般には単語はたくさんある。仮に100個あるとすると $P(A_1, A_2, ... , A_{100}|S)$ は$2^{100}$ 通りの値を持つ。まあ無理だな。
そこで$P(A_1, A_2, ... , A_{100}|S)$を$P(A_1|S)\times P(A_2|S)\times ... \times P(A_{100}|S)$ で近似しちゃう。
- これは「スパムであるという条件下でのA1の出現確率とA2の出現確率は独立である」という仮定(条件付き独立性)を入れてることになる。
こうやって近似すると、$2^{100}$ 通り計算する代わりに、100通り計算して掛け算するだけになってまともに計算できる。

[[tf-idf]]との関係
$P(A_1|S)$は「スパムである文章に単語A1が出現する確率」だった。
tfとは「スパムである文章に単語A1が出現する回数」になる。「出現したら1、しなければ0」という重みの付け方をして、全部足し合わせてから総文書数で割れば$P(A_1|S)$になる。
$P(A_1)$は「任意の文章に単語A1が出現する確率」だった。
[[idf]]は「全ての文章に単語A1が出現する回数の逆数」になる。これも同様にバイナリー重みづけをして足し合わせて総文書数で割れば$P(A_1)$になる。
ベイズ則の分子と分母に現れているので「総文書数で割る」は定数倍だから約分されて消える。


BernoulliNBとGaussianNBの違い
$P(A_1|S)$
っていう確率分布がどういう形か、の違い。
今回の例ではスパムメール中の単語の出現有無だったので「出現する、しない」の0/1の値を取る分布だった。これはベルヌーイ分布なのでBernoulliNBを使うのが自然。
たとえばキノコが毒キノコかどうかを識別したい、というケースでは説明変数が「毒キノコであるときの傘の大きさ」「毒キノコであるときの傘の赤み」みたいな連続の値を取ることもあろう。そういうケースでこの値が正規分布に従うだろうと考えるのがGaussianNB。適当な閾値で0/1に変換してベルヌーイ分布として扱うこともできる。その場合はBernoulliNB。

#機械学習