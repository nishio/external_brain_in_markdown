
> [@taku910](https://twitter.com/taku910/status/1660845546553180163?s=20): 今どきのNLPは生データから教師なし学習する[[サブワード]]を使い言語非依存になっています。英語に特化してるのではなくトークン数の差は学習データの量の差からくるものです。日本語データを増やせば自然にトークン数の差は減ります。日本語知識に「特化」するとかえって多言語処理の弊害になります。
> [@taku910](https://twitter.com/taku910/status/1660847558338523137?s=20): 逆に言えば、この言語非依存のサブワードという技術があるからこそ、コーパス集めりゃ言語に依存せずそれなりに動くことが実現できています。日本語の[[形態素解析]]等は[[多言語化]]の障壁でしかありません。
> [@taku910](https://twitter.com/taku910/status/1660857987815055362?s=20): 豊富な英語コーパスを犠牲にして日本語を増やすのは現実的ではないので、サブワードの学習のときのみ日本語コーパスを水増しすればトークン数問題? はなくなりますが、本当に数が精度に有効なのかは分かりません。

[[日本語LLM]]
