
> [@yuiseki_](https://twitter.com/yuiseki_/status/1644340735867756544): [[大規模言語モデル]]はずっと一貫して入力に対して尤もらしいことを出力しているだけで、たまに事実を出力するなぁとか感じているのは、人間のほうが[[Hallucination]]を見ているとも言えませんか

> [@nishio](https://twitter.com/nishio/status/1644352456854487044?s=20): 「AIが真実を教えてくれる」と妄想して出力を真実だと思い込んだり、真実でないと気づいた時に「AIが嘘をついた！」とAIに非倫理的な意図があったかのように妄想したりするのは人間のハルシネーションと言えそうだ
- [[人間のバグ]]

> [@nishio](https://twitter.com/nishio/status/1644732242974117889?s=20): よく見る人間のハルシネーションのリスト
> 1: LLMが常に真実を出力すると考える
> 2: LLMが真実でないことを出力した時に嘘をつく意図があったと考える
> 3: LLMが学習データより後のことを知ってると考える
> 他に何があるかな？

> [nishio](https://twitter.com/nishio/status/1645611497727606785) 人間のハルシネーションの例「正しい情報で学習させたシステムは正しい情報を出す」
>  >noricoco: 日経Think！今朝はこの記事にコメントしました。多くの記者が「誤情報を学習することで、chatGPTは間違える」と思うようですが、仕組み的に、たぶんそうではなく、出力状態から見て、chatGPT自体が捏造？している可能性が高いかと思います。
>  [https://nikkei.com/article/DGXZQOUC100WA0Q3A410C2000000/…](https://nikkei.com/article/DGXZQOUC100WA0Q3A410C2000000/…)

