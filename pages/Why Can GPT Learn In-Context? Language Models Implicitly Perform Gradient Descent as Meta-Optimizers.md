---
title: "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"
---

> Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design.
[https://arxiv.org/abs/2212.10559](https://arxiv.org/abs/2212.10559)
(DeepL)大規模な事前学習済み言語モデルは、驚くべき[[文脈内学習]]（[[In-context learning]], ICL）能力を示している。数組の入力とラベルのデモがあれば、パラメータを更新することなく、未知の入力に対するラベルを予測することができる。このような性能の大きな成功にもかかわらず、その動作メカニズムはまだ未解決のままである。本稿では、言語モデルをメタ最適化器として説明し、文脈内学習を暗黙の微調整として理解する。理論的には、Transformer attentionが勾配降下の二重形式を持つことを理解する。その上で、ICLを次のように理解する： GPTはまず実証例に従ってメタ勾配を生成し、次にこれらのメタ勾配を元のGPTに適用してICLモデルを構築する。我々は、我々の理解を支持する経験的証拠を提供するために、実際のタスクにおけるコンテキスト内学習と明示的な微調整の動作を包括的に比較する。実験結果は、インコンテキスト学習が、複数の観点から明示的な微調整と同様の振る舞いをすることを示している。Transformer注意と勾配降下との間の二重形式に触発され、我々は運動量と勾配降下との類似によって運動量ベースの注意を設計する。バニラ注意よりも改善された性能は、別の観点から我々の理解をさらに裏付け、さらに重要なことは、将来のモデル設計に我々の理解を活用できる可能性を示している。
