
思考の整理のためのページ

機械学習でモデルを学習した後、何らかの方法で「モデルの評価」を行って「モデルの良さ」(例えば精度など)の値を得ることが広く行われている。さらにその得られた値が良いモデルを選ぶという「モデルの選択」もよく行われている。

これに対する批判「モデルの評価結果を最大化するモデル選択は、評価に使ったデータへの過学習を起こす」

モデル評価の方法(割合は一例)
- A: データの2割を取り分け、残り8割で学習したモデルの性能を、2割のデータで評価する(ホールドアウト法)
- B: データを2割づつの5つに分割する(D1, D2, D3, D4, D5)。 D1+D2+D3+D4で学習したモデルをD5で評価し、D2+D3+D4+D5で学習したモデルをD1で評価し、と5回の評価を行い結果を平均する(クロスバリデーション法)
- C: データを6割、2割、2割に分割する(D1, D2, D3)。D1で学習したモデルをD2で評価する。その後D1+D2で学習したモデルをD3で評価する。
    - 一般的には「パラメータを変えてD1を学習しD2で評価すること」を繰り返してR2が最大となるパラメータを求めた上で、そのパラメータをD1+D2で学習したものをD3で評価する
        - D2で評価した結果(R2)とD3で評価した結果(R3)の使い分けが良くわからない
            - R2を元にモデル選択をするんだったら、R3の値は何に使うのか？眺めて満足するだけ？
        - R3の値を見て「思ったより低いぞ」と言って別の試行をやるのなら、これはモデル選択の一環であって、R2とR3を分けたことにあまり意味がないのではないか。
        - R3の値をモデル選択に使わないなら「試行錯誤に使うデータD2と最終評価専用のデータD3に分けました」という主張は筋が通る。でも実際はモデル選択に使うことが多いのでは？
        - 例えば「D1+D3で学習してD2で評価し、D1+D2で学習してD3で評価し、平均を取る」という形なら「5foldのクロスバリデーションがしたかったんだけど計算コストが高いから2回だけやった」という形で納得できる
        - R3がR2に比べてどの程度悪かったらダメだと考えるのかが明瞭ではない。「D1+D2で学習してD3で評価する」という実験は何を目的として行うもので、どういう状態を成功と定義するのかが曖昧。実験前にそれを決める必要があるのでは？

モデルの選択
- SVMがいいかな、LRがいいかな、という選択
- SVMのパラメータをいくらにしようか、という選択
- NN系において何epochで止めるか(学習epoch数というパラメータをいくらにするか)の選択(early stopping)

- early stoppingでは普通A(ホールドアウト)が使われている
- 計算コストが安いならB(クロスバリデーション)をすればよい
- Cは例えば、Bをすることが計算量的に困難なシチュエーションで、でもAを行った場合に「それはパラメータサーチの段階で検証データにoverfitしているのではないか」とつっこまれることが予想される場合に、「取り分けておいたデータで検証してこの精度が出たのだからパラメータサーチの影響ではない」と反論する時には有用そう。