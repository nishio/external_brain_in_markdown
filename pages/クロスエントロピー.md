---
title: "クロスエントロピー"
---

- p, qが離散的な確率分布であるときのクロスエントロピー:
- $H(p,q) = -\sum_x p(x) \log q(x)$
    - xは多クラス分類の文脈ならクラスに相当する
- xが{0, 1}である時:
- $H(p, q) = -\{ p(0)\log q(0) + p(1)\log q(1) \}$
- 0である確率と1である確率を足せば1なので
- $H(p, q) = -\{ (1 - p(1))\log (1 - q(1)) + p(1)\log q(1) \}$
- pが1である確率をt、qが1である確率をyとして書きなおすと
- $H(p, q) = -\{ (1 - t)\log (1 - y) + t \log y \}$

### クロスエントロピーとLog lossに関する混乱の多くは、sumが何についてのものかをあいまいにすることが原因
- 上記のとおり、xがクラスを走る前提で、2クラス分類なら下記が成り立つ
- $-\sum_x p(x) \log q(x) = -\{ (1 - t)\log (1 - y) + t \log y \}$
- iが各データを走る前提で全データの損失の和を計算するなら下記が成り立つ。(上式をsum_iでくるんだだけ)
- $-\sum_i \sum_x p(x) \log q(x) = -\sum_i \{ (1 - t)\log (1 - y) + t \log y \}$
- この2つを混同して下記2つを並べて比較したりするのは無意味
- $-\sum_x p(x) \log q(x)$
- $-\sum_i \{t \log y + (1 - t)\log (1 - y)\}$
- 「クロスエントロピーとLog lossは同じ！」という表現で上2つの式を同一視したりすると破滅する