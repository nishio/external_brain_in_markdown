---
title: "DeepSeek-R1"
---

[deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file)
[論文(PDF)](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)要約
<img src='https://scrapbox.io/api/pages/nishio/o1 Pro/icon' alt='o1 Pro.icon' height="19.5"/>
DeepSeek-R1は、大規模言語モデル（LLM）の推論力を強化するために、大規模強化学習（RL）を中心に構築されたシリーズです。まず、教師データなしでベースモデルに直接RLを適用したDeepSeek-R1-Zeroを開発し、高度な推論能力を獲得できることを示しました。しかし可読性や言語混在などの問題が残るため、数千件の「長い思考過程付き」教師データ（コールドスタートデータ）を少量導入し、段階的な学習パイプラインを組み合わせたDeepSeek-R1を提案。最終的にOpenAIのo1-1217と同等水準の推論性能を達成しました。

また、学習済みのDeepSeek-R1を「教師モデル」として、QwenやLlamaなどの軽量モデルに蒸留（Distillation）する手法も検討し、小型モデルでも高い推論性能を得られることを示しています。特にQwen-7Bや14B、32Bといった小型モデルが大幅に性能向上し、OpenAIの小型モデルと比肩する結果を達成しています。総じて、RLと蒸留を組み合わせることで、小型かつ高性能な推論特化LLMの開発に道を開く研究になっています。

有識者の意見
> [hillbig](https://x.com/hillbig/status/1881471978252705822) DeepSeek-R1は強化学習により推論能力の大幅な改善を達成。強化学習の報酬のみでo1のような長期的推論過程を獲得。それを小型の密なモデルに蒸留した場合それらも同様な推論能力を獲得した。o1に匹敵するモデルの学習詳細が初めて述べられている例と考えられる。
>
>  はじめに、ベースモデルから強化学習のみでどの程度の能力が獲得できるかを評価した（DeepSeek-R1-Zero）。
>  強化学習として[[GRPO]]を採用。これは価値関数やCritic, Rewardモデルを使わず、グループごとの報酬スコアをもとに方策を最適化する。
>  各質問に対し、G個の出力をサンプリング。各報酬を全報酬の平均を引き、標準偏差でわり正規化したのをAdvantageとした上で、開園後の方策と元の方策との対数尤度比を改善
>  （おそらくRL手法自体は何でも良いと思われる）
>
>  報酬はルールベースの報酬を使った。数学の場合は回答、プログラムの場合はコンパイラの結果やテストケースを通過したかを報酬とした。またフォーマットに従っているかも利用した。後の考察では別の報酬（[[Process Reward Model]], [[MCTS]]）を試したがモデルが報酬をハックしたり、局所解に陥ってうまくいかなかったことが述べられている。
>
>  この強化学習過程で数千ステップで性能は改善し、モデル自身の試行錯誤で新しい解答戦略をみつけた"Aha"モーメントも見つかった。学習するにつれ、複雑な問題を解くために、より長く思考するのも獲得された
>
>  一方、この結果得られたモデルは可読性や複数言語が混ざってしまう問題があった。
>
>  こうした問題からR1は
>  SFT -> 推論タスク向け大規模RL -> タスク向けSFT -> 全タスク向けRL
>  の4工程で学習を行った。
>  最初のSFTは少量の高品質データを使い可読性の向上を行った、続く大規模RLでは言語一貫性報酬も加えている
>
>  密モデルへの蒸留ではこうしたモデルを教師データとして使った。小さなモデルを直接大規模RLするよりも蒸留したほうが性能はずっと高かったことが報告されている
>
>  ==
>  上記結果からはある程度強いモデルでなければ大規模強化学習で自らの性能改善が行えないのではないかと考えられる。
>  また外部報酬が重要であり、今回はルールベースで報酬が与えられる数学やコード、フォーマットなどであった。現在重要となるソフトウェアエンジニアリング向けやエージェントも同様に外部報酬は設計しやすいと思われる

<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>
ある程度強いモデルにおいては、強化学習によってより強くなることができる
- これは「インターネット上のデータを使い尽くしたからもう賢くなれない」的な風説への反論になる
- 数学やプログラミングのような「正解かどうかの判定コストが低いタスク」に関して、それを報酬関数として強化学習をすることで「考え方のコツ」を獲得して正解率が跳ね上がる
- また、ここでできた「賢い大規模モデル」が教師になって小さいモデルを学習することによって小さいモデルも賢くなった
    - ![image](https://gyazo.com/b64092d8b5a2ff91f11fdb5b884de35b/thumb/1000)
    - 14B~32Bでo1-miniを超えている
    - 人間の日常的なタスクの大部分は4oで十分であることを考えると、内容によっては1.5Bで十分となる可能性
        - →LLMの自前ホストの可能性が大きく広がった

> This code repository and the model weights are licensed under the MIT License.

日本語で指示した時に思考プロセスが中国語であるという話
- そんな時もあるのかもだけど僕が試した範囲では英語だった

[https://ollama.com/library/deepseek-r1:32b/blobs/6150cb382311](https://ollama.com/library/deepseek-r1:32b/blobs/6150cb382311)

ollamaでdeepseek-r1:32bを実行
- [https://gist.github.com/nishio/85f621c5944971652228cdab417bb113](https://gist.github.com/nishio/85f621c5944971652228cdab417bb113)
- 手元の単なるMacBookでここまで動くんだな〜という感じ

o1に比べて倫理観のフィルタが弱いという話
- そうでもないと思うな
    - [https://gist.github.com/nishio/222dfb4ddeef551b9e993a49707c4f4c](https://gist.github.com/nishio/222dfb4ddeef551b9e993a49707c4f4c)
