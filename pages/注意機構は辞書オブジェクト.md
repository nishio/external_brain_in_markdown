---
title: "注意機構は辞書オブジェクト"
---

[[注意機構]]は[[辞書オブジェクト]]か？

[[内積注意]]について考える
$Attention(query, Key, Value) = Softmax(query \cdot Key) \cdot Value$

- 高次元のベクトルにおいて、任意の2つのベクトルはだいたい内積が0に近い([[次元の呪い]])
- [[Soft-argmax]]近似を使えば実質的にargmax
- Key→Valueの関数がたとえ学習によって獲得することが困難であっても、記憶によって作ることができる
- 実際の辞書オブジェクトはkeyの一致を判定するが、こちらは内積の近さ
    - ほとんど全ての内積が0だから空間がこんな感じでファジーに分割されるだろう
    - ![image](https://gyazo.com/4913eacee07f06b21aba1e8baa770671/thumb/1000)

[[Key-Value Memory Networks for Directly Reading Documents]]
- [https://arxiv.org/pdf/1606.03126.pdf](https://arxiv.org/pdf/1606.03126.pdf)
