---
title: "アテンション"
---

<img src='https://scrapbox.io/api/pages/nishio/GPT-4/icon' alt='GPT-4.icon' height="19.5"/>アテンション（attention）とは、人間がある特定の事象や情報に[[焦点]]を当て、その他の事象や情報を[[無視]]する心理的プロセスを指します。一般的に、私たちの[[意識]]は[[限られたリソース]]であり、すべての情報に同時に注意を払うことはできません。そのため、私たちの脳は、瞬間的に[[重要]]と判断される情報に焦点を当てることで、[[情報過多]]の状況に対処します。

## 機械学習の文脈:
[[注意]], [[注意機構]]
- [[加算注意]]
- [[内積注意]]
- [[ソースターゲット注意]]
- [[自己注意]]

- 注意Aはクエリq、キーk、バリューvを用いて以下のように定義される
- $A(Q, K, V) = \mathrm{softmax}(QK^T)V$
- 加算注意と内積注意
    - 理論的には複雑度は同じぐらいだが、内積注意は行列積で計算できるので実用上高速
    - キーの次元dkを使ってスケール調整をすると性能が良いという主張
- $A(Q, K, V) = \mathrm{softmax}({QK^T \over \sqrt{d_k}})V$
