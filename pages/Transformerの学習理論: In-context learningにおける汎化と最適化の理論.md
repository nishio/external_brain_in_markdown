---
title: "Transformerの学習理論: In-context learningにおける汎化と最適化の理論"
---

[NLPコロキウム | Transformerの学習理論: In-context learningにおける汎化と最適化の理論 (鈴木大慈)](https://nlp-colloquium-jp.github.io/schedule/2024-09-25_taiji-suzuki/)
> [[鈴木 大慈]] / Taiji Suzuki (東京大学)
> [[In-context leaning]]を主たる題材として，[[Transformer]]の学習能力を理論的に明らかにする最近の理論研究を紹介する．まず表現力の理論として，Transformerは[[非等方的滑らかさ]]を持つ関数を近似できること，および[[自己回帰的データ]]を学習できることを紹介する．同様の結果が[[状態空間モデル]]を用いても実現できることを紹介する．次に最適化理論として，[[非線形特徴学習]]の最適化が可能であることを示し，真の関数の[[情報指数]]によって計算効率が評価できることを示す．時間があれば，統計理論としてin-context learningにおいて[[minimax最適性]]を満たすことも紹介する．

![image](https://gyazo.com/2467b4d2204986d5ad3c234aacda2220/thumb/1000)
[[線形注意]]
- [[Linear Attention]]
