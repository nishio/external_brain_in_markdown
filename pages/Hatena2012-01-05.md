
hatena

```
<body>
*1325752757*IBMモデル1に言語モデルを追加した
先日の<a href='http://d.hatena.ne.jp/nishiohirokazu/20120104/1325667448'>IBMモデル1の実装</a>に関して、なんで「house house house」ってなってしまうのかについて考察した。与えられたデータだけからは、dasがhouseなのか、Hausがhouseなのか、istがhouseなのかわからない。だから確率値もイコールになっていて、argmaxした際に最初に見つかったものが選ばれてhouseになっているものと思われる。

前回の実験では言語モデルが入っていないから、「ここにはtheとかhouseとかisとかが入りそうだ」と思っても、それをどう並べればいいかについて情報がない。じゃあ言語モデルを入れたら改善されるんじゃないか？

>||
# 前回の結果
das Haus ist klein
house house house small
das Haus ist schmutzig
house house house dirty
das Haus ist sauber
house house house clean
||<

で、2-gramの言語モデルを入れたらどうなったかというと、教師データに関しては全問正解するようになった。しかし教えてない文に関してはこんな感じ

>||
das Stift ist klein
the a a a
Ich habe klein Stift
I have a pen
Ich liebe Haus
I love a 
||<

なぜこうなったのか考えてみたが、n-gramでカウントする際にスムージングをしていなかったから「教師データに出現していない単語の並び」が出たら「0回出現だから確率0」と判断して、確率0のものの中から適当に「a」を選んだんだろう。じゃあスムージングしてみよう。

>||
das Stift ist klein
the a house small
Ich habe klein Stift
I have small a
Ich liebe Haus
I love house
||<

さっきのよりはだいぶ良くなった。しかし、1つめはthe pen is smallだし、2つめはI have small penだしなー。最後のI love houseはまああってる。

IBMモデル2も実装してみたんだけども、これ入力の単語数lfと出力の単語数leごとに(lf, le)のdistortion matrixを作って学習するとか言っていて「そんなことをしたら現状の手作りデータでは全然足りないじゃないかー」と思ってまだ試していない。そろそろ手作りしてないで適当なコーパスを使うべきかも。全体像を把握できなくなるのが嫌なんだよなぁ。

ところで、そもそもdistortionが単語とかに全く依存していない「この長さの文章だったら3番目と4番目を入れ替えろ」みたいな学習の仕方で、こんなのでは無闇に次元を挙げて次元の呪いに突入してしまうんじゃないかと懸念。mosesなんかではdistortionをどこまで認めるかをオプションで指定できたと思う。欧米言語の間での翻訳はdistortionを小さく制限してもOKなのかもしれない。でも英和をするときには無制限にするよう指定した記憶があって、これって深く考えずに指定したらまずいんじゃないのかなぁーなどと思うなど。

やっぱり「<a href='http://d.hatena.ne.jp/mamoruk/20110606/p1'>論文は実装すると理解が深まる</a>」は正しいなぁ。実は今やっている実装マラソンはこのエントリーを見たことがきっかけだったのです。
</body>
```


[はてなダイアリー 2012-01-05](https://nishiohirokazu.hatenadiary.org/archive/2012/01/05)