
- 「k-NNの前にPCAで次元削減をすると精度が上がる」→NG
- 「PCAで次元を削減すると計算に掛かる時間が減る」→OK
- 「教師データを増やせば精度が上がる」→OK
- 「元データの相関が高い場合に、PCAで次元削減をしても、精度は下がりにくい」→OK
    - 「元データの相関が高い場合に、PCAで次元削減をしても、精度は下がらない」→NG
    - 「元データの相関が高い場合に、PCAで次元削減をすると、精度が上がる」→NG
- 上記の合算により「元データの相関が高い場合、k-NNの前にPCAで次元削減をしても精度は下がりにくく、計算にかかる時間は減るので、その分教師データを増やせば精度が上がる」→OK

- 「PCAは回転なので、次元削減しないならk-NNの精度は変わらない」→OK
- 「ある軸が原因でオーバーフィットしている時に、その軸を切り落とせば、汎化性能が上がる」→OK
    - 「PCAの『分散が少ない軸』が、オーバーフィットの原因である」→その確率はゼロではないが、他の軸に比べて低い
    - PCAで捨てられる軸は「教師データのラベル情報yをまるっと無視した上で、xの分散が小さい方向」であり、「識別する上で重要度が低い」とか「識別する上でノイズになる」とかではなく、単に「xがその方向にはあまり動かない」にすぎないから。
- 「k-NNは元データのスケールの影響を大きく受ける」→OK
    - 「なので正規化すると精度が上がる」→NG
        - 正規化とは「各軸の分散が1になるように調整すること」で、それは「すべての軸が均等に重要であるにも関わらず、センサの値域の都合などで分散に差が出ている場合に、分散の小さい軸が不利にならないようにする」ということ。
        - PCAは「軸x1と軸x2は相関が強いから、x1 + x2の分散は大きく、x1 - x2の分散は小さい。これを新しい軸にしよう」という軸回転の手法で、回転の後「で、分散の小さいx1 - x2の軸は無視しよう」と次元削減が組み合わされることが多い。分散の小さい軸の一部を無視して、残りを拡大するのは、割りと変な行為。