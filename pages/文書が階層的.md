
![image](https://gyazo.com/b97714f6de37e6a90043ceee1feea38c/thumb/1000)
- 書籍の各ページを対象文書とする場合
    - 各対象に出現するキーワードはDFが大きいので[[TFIDF]]は小さくなる
- 書籍を一つの対象文書とする場合
    - 書籍内のいくつものページに出現するキーワードはTFが大きいのでTFIDFが大きくなる
- つまり対象の輪郭によって逆方向の影響を受ける
    - 対象の輪郭によらない尺度はないか？

- [カーネル密度推定 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E5%AF%86%E5%BA%A6%E6%8E%A8%E5%AE%9A)
- ${\displaystyle {\hat {f}}_{h}(x)={\frac {1}{nh}}\sum _{i=1}^{n}K\left({\frac {x-x_{i}}{h}}\right)}$
    - 適当なウィンドウで密度推定をした場合に、本当に一様に出現するものなら一様分布になるはず
    - そこからの分布の距離を見れば良いのではないか
    - 分布の距離は[カルバック・ライブラー情報量 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F)で良いか
    - ![image](https://gyazo.com/49118c094aa54a2aeb477367a37cd005/thumb/1000)
    - しかも片方の分布が固定
    - 大小関係を考えるだけならQを無視して良いので
    - $\sum P(i) \log P(i)$
    - あ、これ[[負のエントロピー]]では
        - [[エントロピー]]([[平均情報量]]の意味での)
        - $-\sum P(i) \log P(i)$

- [[接尾辞配列]]が作られているとする
    - あるキーワードの出現位置は、そのキーワードで始まる接尾辞の出現位置を見れば分かる
    - そこから密度推定ができないか？
        - もしくは密度推定を飛ばして直接エントロピーを計算できないか？
    - 想定しているデータサイズ
        - 書籍1000冊分+ブログなど、1GBいかないぐらい
    - 雑な方法
        - 文書全体を適当なサイズのビンに割っておいて、キーワードの出現数をビンごとに数える
        - ビンを10000としてキーワードを最長50文字、カウントを2バイトとしても大した量ではない
        - この数える過程はO(N)
        - 最後にエントロピーでソートして結果を見る

