---
title: "CTM"
---

from [[BERTopic: Neural topic modeling with a class-based TF-IDF]]

<img src='https://scrapbox.io/api/pages/nishio/claude/icon' alt='claude.icon' height="19.5"/>
CTM (Contextualized Topic Model) は、BERTopicと同様に、事前学習済み言語モデルを活用した最新のトピックモデルの一つです。CTMは、BERTopicが発表される少し前の2020年に提案されました。

CTMの主な特徴は以下の通りです。

1. BERTの文脈埋め込みの活用
CTMは、BERTなどの事前学習済み言語モデルを使って、単語の文脈情報を取り込みます。具体的には、各単語をBERTに入力し、その出力の平均をとることで、文脈を考慮した単語埋め込みを得ます。

2. VAEベースのトピックモデル
CTMは、変分オートエンコーダ（VAE）をベースにしたトピックモデルです。VAEは、潜在変数モデルの一種で、データの生成過程をモデル化することができます。CTMでは、トピック分布とトピック-単語分布を潜在変数として扱い、VAEを使って推定します。

3. 単語の文脈情報と出現頻度の統合
CTMでは、単語の文脈情報（BERTの埋め込み）と出現頻度の両方を使ってトピックを学習します。これにより、文脈的に関連する単語が同じトピックに割り当てられやすくなります。

4. end-to-endの学習
CTMは、トピック分布の推定とトピック-単語分布の推定を同時に行います。これにより、トピック分布とトピック-単語分布が相互に影響を与えながら最適化されます。

CTMは、いくつかのベンチマークデータセットにおいて、従来のLDAやProdLDAなどのトピックモデルを上回る性能を示しています。特に、トピック一貫性とトピック多様性の両方で優れた結果を達成しています。

ただし、CTMはBERTopicと比べると計算コストが高く、大規模データセットに対しては適用が難しいという問題点があります。また、BERTopicのようなクラスタリングベースのアプローチとは異なり、トピック数を事前に指定する必要があります。

以上のように、CTMはBERTopicと並ぶ最新のニューラルトピックモデルであり、言語モデルを活用してトピックの質を向上させる点で共通していますが、アプローチの詳細は異なります。それぞれの長所と短所を理解した上で、タスクに応じて適切なモデルを選択することが重要だと言えます。
