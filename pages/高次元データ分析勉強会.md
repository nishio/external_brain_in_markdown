---
title: "高次元データ分析勉強会"
---

2024-11-22 [[サイボウズラボ勉強会]]
- 前回は2024-10-18 [[世論地図勉強会]]
    - [[2024衆院選]]における人々の意見ベクトルを収集して可視化するサービス「[[世論地図]]」をリリースした日だった
- これは最終的に 4403 rows × 84 columns のデータになった
    - (オープンデータ化を進めているところなのでできたらここに書く)
- 今回はこれの分析も含めた「高次元データの分析」について解説する
- ざっくり目次
    - 次元削減の歴史(PCAからUMAPへ)
    - 2022年参院選のUMAP可視化実験
        - いい感じ
    - [[世論地図のUMAP]]実験
        - 高次元泥団子との格闘
        - 悩む→気づき
    - 新しいプロジェクト(11/22公表予定)
    - [[濃い塊の抽出]]
    - KJ法との関連


次元削減の歴史(PCAからUMAPへ)
- PCA(主成分分析)は線形な次元削減の方法
    - 1901年に[[カール・ピアソン]]（[[Karl Pearson]]）が提案した古典的手法
- 20世紀中盤：PCAの限界（線形性に依存）が指摘され始め、非線形な構造を扱う手法の必要性が高まる。
- 1960年代: [[多次元尺度構成法]]（MDS, Multidimensional Scaling）
    - 距離行列を元に高次元データを低次元空間に写像。
    - > 古典的MDSは[[主座標分析]] (Principal Coordinate Analysis; PCoA) とも呼ばれ、さらに主座標分析において距離にユークリッド距離を用いた場合は主成分分析と等価になる。 --- [多次元尺度構成法 - Wikipedia](https://ja.wikipedia.org/wiki/%E5%A4%9A%E6%AC%A1%E5%85%83%E5%B0%BA%E5%BA%A6%E6%A7%8B%E6%88%90%E6%B3%95)
    - 線形版と非線形版があり、非線形MDSは非ユークリッド距離を扱える。
- 1982年: [[自己組織化マップ]]（[[SOM]], [[Self-Organizing Maps]]）
    - 実は僕の20年前の博士号の研究はSOMの拡張の一つだったので、ChatGPTに「非線形次元削減の歴史を書いて」と言ってこれが出てきた時には「おっ、懐かしい顔だな」という気分だった
        - 「神経科学的アプローチに基づき、データを低次元の格子にマッピング」とChatGPTは説明してて、まあ世の中一般的にはそれでも間違いではないんだが、この時期によくある「脳の神経接続にインスパイアされて作ったモデル」で、近似しすぎてもはや脳の中の挙動とあんまり関係ない
            - まあ今のニューラルネットもだいぶ神経関係ない
        - k平均法に代表点の間の隣接関係/距離をいれたもの
            - データのトポロジカルな特性を考慮した解析手法として先駆的役割
        - 20年前の時点では非ユークリッド距離への拡張が行われていた
- 1998年: カーネルPCA（Kernel PCA）
    - [[カーネル法]]を用いて非線形構造を扱う。
    - [[再生核ヒルベルト空間]]（RKHS）でPCAを適用。
    - [[サポートベクターマシン]](1992)の隆盛からの派生の一つ
    - カーネルの概念は深層学習の数理との関係で再注目されつつある
- 「データのトポロジカルな特性を考慮した解析手法」の発展
    - 2000年: [[Isomap]]（[[Isometric Mapping]]）
        - 距離構造をグラフに基づき近似し、多様体構造を保った次元削減。
    - 2000年: [[LLE]]（[[Locally Linear Embedding]]）
        - データの局所的な線形性を利用して次元削減を実現。
- 2008年: t-SNE（t-Distributed Stochastic Neighbor Embedding）
    - 高次元空間の局所的な類似性を確率的に表現し、低次元に写像。
    - 可視化に特化した手法として広く普及。
- 2018年: UMAP（Uniform Manifold Approximation and Projection）
    - t-SNEの欠点（計算負荷や局所構造の崩れ）を改良。
    - 現代の可視化と次元削減手法の主流。
- UMAPの詳しいアルゴリズムの説明をしようかと思ったけど、それだけで1時間なくなりそうだからまた今度にすることにした
    - このセクションのまとめ
        - PCAは1901年の古典的次元削減手法
        - 線形であることによるデメリットがあるので非線形次元削減手法が長年研究されてきた
        - 今は2018年のUMAPを使うのがいいとされている
- PCAではダメなケース
    - ![image](https://gyazo.com/188215f1b3ee60ca4f81903b19984082/thumb/1000)
    - [[対立次元]]
        - 1: AとBが対立している
        - 2: (A+B)とCが対立している
        - 3: ((A+B)+C)とDが対立している
            - ここで次元数が足りなくて2次元の[[PCA]]ではAとBが同一視されるようになる
            - 非線形の次元削減([[UMAP]]など)では分かれる



[[2022年参院選のUMAP可視化]]
- 今までの経験的にPolisは進行するに従ってクラスタが分かれなくなっていく傾向があると感じていた
    - 今回の世論地図でも終盤に全部クラスタが分かれなくなったら嫌だなと思っていた
    - なのでオリジナルのPolisのPCAを使う次元削減ではなく非線形のUMAPを使うバージョンを検討していた
    - なお今回の世論地図の実験においては「進行するに従ってクラスタが分かれなくなっていく傾向」が観測されなかった
        - この現象は投票ユーザ数が増えていくことによる現象ではなく、ユーザが新しい意見を投稿することによって意見空間が高次元化していくことが原因なのかもしれない
        - 詳細な議論: [[Polisクラスタ2つになる問題]]
- 世論地図のデータが出る前に、予備実験として以前PCAで分析した2022年参院選データ([[東京大学谷口研究室・朝日新聞社共同調査]])を[[UMAP]]で可視化してみた(592人、42質問)
    - 左: UMAP [[2022年参院選のUMAP可視化]] / 右: PCA [[2022年参院選のPolis的可視化]]
    - ![image](https://gyazo.com/2d34ad06a7c1206fc84cc7292123700d/thumb/1000)![image](https://gyazo.com/77f9fc60c241159a0062e4842a7435cf/thumb/1000)
    - PCAの方は点に注目すると、ほぼただの楕円になっている
        - そのままではあまりに意味がわからないので党を[[Convex Hull]]で囲って色を塗ってある
        - これが「PCAでクラスタが分かれなくなる」の意味
        - とはいえ分かれてないこれでもすごく面白いらしくて2024-11-16の[[Code for Japan Summit 2024]]のKeynoteでも安野さんが紹介してくれてたらしい
        - ![image](https://gyazo.com/b5cbb13a30bb7cc903d0cabae78a7040/thumb/1000)
        - ![image](https://gyazo.com/9efdf13860b4680989cac0361b5f0e97/thumb/1000)
        - PCAなので軸に意味がある点はメリット
            - 非線形の手法では軸が曲がりまくるので今の解釈が無理になる
- PCAでほぼ楕円の1クラスタになっていることに比べてUMAPでの分析はすごくはっきりとクラスタが分かれている
    - ![image](https://gyazo.com/2d34ad06a7c1206fc84cc7292123700d/thumb/1000)
    - 赤枠は詳細解説のために手動でつけたもの
        - クラスタごとの詳細解説はこちら: [[2022年参院選のUMAP可視化]]
    - 特定の政党によって占められている濃い島と、複数の政党が混じりあっている島とがある
        - つまりA, B ,Cの党があるとき、意見の分断は党の間にあると考えがちだが、実際には党内の過激派と穏健派との間に分断があるわけ
        - ![image](https://gyazo.com/3916e82cf21628e1fd4e0e54177dda48/thumb/1000)
- これは結構面白い結果になったと思ってる



[[世論地図のUMAP]]
![image](https://gyazo.com/30c9d0303e0de3e01269148c72184c1e/thumb/1000)
- 世論地図のデータが取れたのでさっそくUMAPしてみた
- [[世論地図3970人UMAP]]
    - 3970人x84次元(すべての地図のデータを結合した場合)
- 2022年参院選データと違って政党データがないので[[DBSCAN]]でクラスタリングしてみた
    - DBSCANはデータベースのスキャンとは全く関係がなくてDensity-Based Spatial Clustering of Applications with Noise
    - 密度ベースのクラスタリング手法(後で詳しく話すのでここではこれくらいで)
- 13個ほどの飛び散ったクラスタと、中央の[[高次元泥団子]]がある
- データ量の比率は？
    - 全部で3970件、周囲の小さいクラスターが655件
    - 84%のユーザが明瞭に分離してない中央クラスターにいる
- おそらく記名アンケートに政治家が答えてるケースでは所属する党の主張との関係性や、野党の与党に対するポジショントークや[[集団極性化]]など「意見が先鋭化する力学」がある
    - [[匿名の市民の意見ベクトルはそんなに明瞭に別れてない]]



高次元泥団子との格闘
- 中央のこの部分、もう少し上手く分析できないだろうか？
    - ![image](https://gyazo.com/ca78ba9feeb5beabee68258e4128cdc9/thumb/1000)
    - 意味ありげな内部構造があるように見えるよね
- この部分の分析を試行錯誤していた([[UMAPの結果をクラスタリングするべきか]]、クラスタリングしない方がいいのでは、と考え始めた)
    - ![image](https://gyazo.com/2ede642899c3f91ba27476630daa82f7/thumb/1000)
    - 灰色の「外れ値」が少なくなるようにパラメータepsを増やすとクラスタは併合されてしまう
    - ![image](https://gyazo.com/868a75c921455b846d041acd1ca2a45c/thumb/1000)![image](https://gyazo.com/8f47923a4850a422f8cd1d6b127b5db3/thumb/1000)
    - これはDBSCANの「外れ値」を「離れている異常値だから捨てたデータ」と解釈することの方に問題があって「二つのクラスターの狭間にあってどちら側とは言いづらいデータ」という解釈にした方がいい
        - ![image](https://gyazo.com/a7b9acd91aa50cf79544b11fff78fde0/thumb/1000)
        - 「AとBはわけたい」「ではどこに境界線を引くのが適切か？」
            - この問がそもそも間違ってるのでは？という話
            - 境界線を引くということは太さのない線によって分割可能であるという前提を入れている
            - 実際には世の中の多くのものは明瞭な境界線を持たず、幅のある「どちらともいいづらいゾーン」を挟んで分かれている
    - (後から理解したがこれはDBSCAN/HDBSCAN系の設計思想を再発明してた)
- この話を安野チーム内で共有したら興味深い話を教えてもらった: [[tSNEの結果のクラスタリングは慎重に]]
- UMAP後のデータではなく高次元空間上で直接クラスタリングを試みた
    - ![image](https://gyazo.com/dcd3a244de09a631bc23d62aece80449/thumb/1000)
    - この結果をみて衝撃的だった
    - ![image](https://gyazo.com/8648c886f5ef2c68dc9cb97c5db24fc4/thumb/1000)
    - つまりどう言うことかというと
        - UMAPでの二次元可視化した後での「クラスタっぽい密度の濃い塊」(青と紫)
        - ![image](https://gyazo.com/ca78ba9feeb5beabee68258e4128cdc9/thumb/1000)![image](https://gyazo.com/5eb99e21a33cd7a7e65d68bfb98457aa/thumb/1000)
        - これが実は高次元空間では全然密度高くないということ
- 何が起きているのか？



[[賛成反対データのUMAP]]の問題
- ![image](https://gyazo.com/66f90b8311c1343c4720c730623b1ca1/thumb/1000)
    - 賛成でも反対でもないデータの量が多いほど赤くしたもの
    - 「クラスタっぽい密度の濃い塊」は欠損値の少ないデータの集まりで、周囲の複雑な意味ありげな形だったものは欠損値多めのデータだった
- 元々このデータは`matrix.dropna(thresh=3)`(欠損してないデータが3件以上なものだけ残す)してたのだけど、ここを`matrix.dropna(thresh=8)`に変えたところUMAPの結果は下記のようになった
    - ![image](https://gyazo.com/cfdedf73e89a82d60ef898acd424606f/thumb/1000)
    - 複雑な形状はほとんどすべて欠損値が原因のアーティファクトだったというわけ
    - 再掲: [[匿名の市民の意見ベクトルはそんなに明瞭に別れてない]]
- 多分こういうこと:
    - 欠損値を平均値で埋める処理によって「ほとんど投票してない人」によるやたら強いクラスタができてしまっている
    - ちゃんとたくさん投票した人は高次元空間に散らばって「薄くて明確な境界のない一塊」になっている
    - UMAPは近傍k点までの距離を使って密度を推定する、これがデータが離散的で同一座標にデータが重なる賛成反対データの特性と干渉して「実態以上に濃いクラスタ」として認識され、UMAPはその特徴的クラスタを分離して可視化しようとした
- しっかり検証はできてないが、一旦「離散的な値の賛成反対ベクトルはUMAP向きではないな」と判断してこの路線はペンディング
    - 全地図の84次元のデータではなく1地図だけのデータで欠損値を含む人を全部捨ててUMAPしたら一応わかれはした
        - ![image](https://gyazo.com/757f3430e3b2aec806bc9a31117dc920/thumb/1000)

    - これも[[Polisクラスタ2つになる問題]]の否定的観察結果についての「クラスタが分かれなくなるのは投票ユーザ数が増えていくことによる現象ではなく、ユーザが新しい意見を投稿することによって意見空間が高次元化していくことが原因」という仮説を支持する



新しいプロジェクト(11/22公表予定)
- これは自然言語の埋め込みベクトルを対象とした分析
- データ分布の特徴が重要だとわかったので類似のデータで調べてみた
    - [[テキスト埋め込みベクトルの分布]]
        - Xから取得した18129件からTalk to the Cityのextructionで抽出されたargument 7574件
        - [[text-embedding-3-large]]で埋め込んで3072次元
        - ほぼ均一距離の単一の塊の中にところどころ濃いところがある
            - 概念図: [[小さな粒のある一つの塊]]
                - ![image](https://gyazo.com/3f553409f14a1b3dd040101a7c102a6d/thumb/1000)
- [[クラスタリングとパーティショニング]]
    - 先ほどの「どこに境界線を引くのか」の問題
        - ![image](https://gyazo.com/80377b55bf03bd7d9ff2cae6ff49cc34/thumb/1000)
        - 左図のような密度のピークの間に何もない密度分布ならクラスタリングに悩むことはない
        - 現実のデータは右図のように密度のピークの間にもデータがあって明確な境界がない
    - 従来の[[K平均法]]などの「[[クラスタリング]]」はデータセットを「分割」しようとする
        - その「分割」(パーティション)をクラスタと呼んでいる
        - ![image](https://gyazo.com/a68806e0a41cf35ff092174d3580ce7f/thumb/1000)
        - XのようなAのピークから密度薄いゾーンを隔ててかなり離れてるデータでも「Aのクラスタ」とされてしまう
        - DBSCANの作者らはこれを問題視した
        - そもそも「クラスタ」とはなんであるのか？
    - DBSCANの作者らが定義し直した「クラスタ」の概念
        - 「クラスタとは、データポイントが密に分布している領域である」
            - > a cluster is defined to be a set of density-connected points which is maximal wrt. density-reachability.
        - ![image](https://gyazo.com/e5281054f0acd540ae656eebb2362ad7/thumb/1000)
            - クラスタはデータポイントが一定密度以上の領域だけであり、Xはどのクラスタにも属さないノイズである、という考え方
        - この二通りのクラスタリングに対する考え方を区別するために彼らは前者を「パーティショニング」と呼び分けている
- Talk to the Cityを使った意見ベクトルのクラスタリングは「パーティショニング」
    - ![image](https://gyazo.com/c2c76475e5a518a3fe868e76c588a42d/thumb/1000)
        - [[SpectralClusteringとHDBSCANの違い]]
        - Spectral ClusteringはTalk to the Cityが可視化目的で使ってるもの
        - HDBSCANは「階層的DBSCAN」
    - [[日テレNEWS×2024衆院選×ブロードリスニング]]での議論を眺めながら「密度の高いところだけ切り出して『Xは低密度のデータ』として捨てた方が下流の『[[AIによるクラスタ解説]]生成』などが上手くいくのではないか？」と考えていた
    - その後でDBSCANなどの問題意識を知って、なるほど同じだな〜と思った



[[濃い塊の抽出]] / [[濃いクラスタ抽出]]
- DBSCAN/HDBSCANがちょっとわかるようになったのでその方針でTalk to the Cityのアルゴリズムをいじることにした
    - 実験ログ: [[pTTTC2024-11-12]]
- Xから取得した18129件からTalk to the Cityのextructionで抽出されたargument 7574件
- [[text-embedding-3-large]]で埋め込んで3072次元
- ここからHDBSCAN(min_cluster_size=5, max_cluster_size=30, min_samples=2)で密度の濃いところを抽出すると79個得られた
    - この設定は細かいクラスタをたくさん拾う意図
    - 目視で「確かに濃い内容だ」となった
- クラスタに使われたデータは739件、カバレッジは9.8%
    - 9割がノイズだという判定
    - ノイズのことを[[外れ値]]と呼ぶこともあるんだけど、外れ値っていうと相対的に少ないと思いこんでしまいそう
- このやり方には現状のTTTCに比べてメリットデメリットがある
- 全体像を散布図と強制クラスタリングで示す現在のTTTCは[[まずは大雑把に]][[全体像を把握したい]]という欲求を満たすのに向いている
    - 都知事選の時はそれを黙ってネットの向こうの人に示す or 政策チームが読んで参考にする と言う感じだった
        - 政策チームからは別手法[[Talk to the City Turbo]]の方が詳細に掘り下げられていて参考になるという意見もあった(N=1)
    - [[日テレNEWS×2024衆院選×ブロードリスニング]]では視聴者に向けて「解説」をする必要性があった
        - その過程で全体像を示すだけではなく「濃い意見を掘り下げたい」というニーズが明確化した
            - このプロセスにおいて現状のTTTCでは「点にマウスホバーして見せる」になってN=1で示すことしかできない
            - また、その「フォーカスする意見を探す」のところに支援が乏しい
                - 適当にいくつかだけ見て運任せにするのか
                - 7000件を結局全部見るのか
        - 今回開発した新しい手法はここを改善する
            - 「7000件全部見る代わりに、濃度の濃い79クラスタを見る」という負担軽減
            - 「N=1ではなくN=5~30の意見」として示せることによる説得力向上
- 公開できる[[TTTC: AIと著作権に関するパブリックコメント]]のデータにこのアルゴリズムを適用して公開した
    - [https://github.com/nishio/ai_kj/blob/main/dense_cluster_extruction.ipynb](https://github.com/nishio/ai_kj/blob/main/dense_cluster_extruction.ipynb)
    - 5957件 1536次元
    - 48個のクラスタが抽出された
    - クラスタの興味深さをGPT-4oに採点させたが、それ自体はあまり面白い結果にならなかった
        - まあこれはAIに「どういう目的で調べててどういうものが興味深いのか」を教えてないからという側面もありそう
    - 出力されたクラスタ一覧に対して"他にない視点を導入しているクラスタとその独創的な点を列挙しなさい"のプロンプトを入れた方が面白い
        - ![image](https://gyazo.com/6f6d83e0fb6c3fda4c88b197727d82f0/thumb/1000)
    - [ChatGPT log](https://chatgpt.com/share/6735c2e5-5bac-8011-bccc-d8ffd64fd1d9)



KJ法との関連
- 6月の[[Talk to the City勉強会]]でも付録で[[KJ法]]との関連について書いた
    - ![image](https://gyazo.com/973ea9617662cabdccfcf594eff6bb7c/thumb/1000)
    - [[発想法]] p.77
    - > 自分の心のなかに、「これだけの紙きれの資料は、自分の考えによれば、内容的に市場調査・品質管理・労務管理と三つに大きく仕切るのが正しい」などというたぐいの、グループ分けについての独断的な原理をあらかじめ頭の中にもっているからである。その独断的な分類のワクぐみを適用し、そのできあいのワクの中にたんに紙きれの資料をふるい分けて、はめこんでいるにすぎないのである。これでは KJ法の発想的意義はまったく死んでしま う。
    - [[川喜田二郎]]は人間が独断的な分類のワクぐみへのふるい分け、はめこみをすることを批判した
    - その対処法として1枚1枚をみて関連するものとくっつける[[KJ法]]を提案した
    - これはデータサイエンスの言葉で言うなら[[凝集型階層的クラスタリング]]
    - TTTCでは、人間ではなく機械がクラスタリングを実行することで「人間が独断的な分類へのはめこみをしてしまうこと」が避けられている
- 今回、より一層KJ法との関係性が明確になってきた
- 「パーティショニング」の意味のクラスタリングはデータセットをトップダウンに分割してしまっている
- DBSCANのような方法で濃度の高いクラスタを抽出するのは、KJ法において「関連するものを集めてグループを作ること」とほぼイコール
- クラスタにうまく入らないものをクラスタに入れずに残すのはKJ法の「[[離れザル]]」と同じ
    - データの9割が離れザルになるというのはKJ法的ではない
    - この現象は今回の手法が「近さ」を「埋め込みベクトル空間での距離」で定義しているから
    - 人間がKJ法をするときにはつながりそうにないものにつながりを見つけてグループにしろという要求で「このXとYは一見つながらないが、Aという[[側面]]では関係している」という「関係の発見」を行う
        - つまりこれは「埋め込みベクトル空間のコサイン距離」の意味で「遠い」ものに対して追加の演算を行うことで別の意味の「近さ」を見出しているわけだ
        - KJ法が「発想法」「知的生産術」として発想の促進を起こすのは、この「関係性の探索」を人に強いるからだと思う
    - 埋め込みベクトルの距離で表現されてない関係性の探索を機械が行うにはどうすればいいか？
        - 拙著「[[word2vecによる自然言語処理]]」では「[[概念の類似度は距離ではない]]」として、ランダムに軸を潰すと近いことになるのではという考察をした
        - 今回のケースにおいても、ランダムに軸を潰す or PCAで寄与率の低い軸を潰す or いっそ発想の逆転で寄与率の一番近い軸を潰す で対処できる可能性がある(今後の実験)
        - むしろLLMのコンテキストに入るならLLMに人間同様の「関係の発見」をさせることができそう
            - LLMのコンテキストに入る量に刻んで良いと思う
            - なぜなら人間の視野に入るラベルの枚数に限界があるのと同じだから
- KJ法的には離れザルを1枚も捨てることはない
    - これを実現する上では離れザルをどうやってクラスタに合流させるかを考える必要がある
    - たぶん単なる距離ベースでは「動きはするが、いい結果にはならない」だろうと思う
    - LLMにプロンプトしてできるかどうかの実験が必要
- KJ法は、まずそもそも熟練が必要な技術でありながら、世界一熟達している川喜田二郎が「[[50枚のKJ法は10時間の想定]]」とか言って、時間がかかりすぎるので[[5年間で129枚しかKJ法をしていない]]という超重量級手法なわけだが、それがAIによって手軽な手段になる時が近づいているように感じた
