---
title: "Hatena2014-02-10"
---

hatena

```
<body>
*1392026316*Deep Learning論文紹介「Deep learning via Hessian-free optimization」その3
残念なお知らせですが、この連載は続きません。

そもそも僕の興味はword2vecによる意味の理解と、言語モデルによる文章の生成だったわけです。後者の論文はRNNLMを使っていて、それは前者の論文の著者がword2vecの前にやっていたことです。というわけでRNNLMについて調べ始め、その最適化が他のニューラルネットでよく使われているような確率的勾配法では難しいということでヘシアンフリー最適化について調べていたわけです。

<a href='http://d.hatena.ne.jp/nishiohirokazu/20140209/1391874480'>前回</a>説明したように、たしかにニューラルネットで重みがほとんど同一であるニューロンがあった場合に、そのニューロンを分化させる方向の勾配が小さくて1次の勾配法では学習がはかどらないという点は理解できます。でも「だから2次の最適化法を使う」という結論にあまり同意できません。個人的には「スパース化の制約をかけたらそのニューロンのどっちかしか発火できない関係上、分化がスピーディに起こるでしょ」と思うわけです。最終的にスパースでないものが欲しいのだとしても、最初はスパース制約をかけて「よい初期値」を探したらいいんじゃないの。「それでよい」や「そっちの方が良い」は仮説にすぎないので、その仮説が正しいかどうかを検証してみるのも一興だとは思うのだけど、実はあんまり興味が持てません。

もう一点、word2vecに喰わせるために単語のリストを作るところで、形態素解析を使いたくない。そこでRNNLMから「単語」を発見する研究がないかなと調べていたところ、むしろ「パラメータ調節が最小限で deep learning による結果とほぼ同等の精度が得られる手法」があるという記述を発見（<a href='http://d.hatena.ne.jp/mamoruk/20130913/p1'>SIGNL 213: 現実的な形態素解析器の入力→「ずももももぺろぺろぺろぺろマミタスマミタスラブマミタス」 - 武蔵野日記</a>）

というわけで関心がこちらの論文に移ったので連載は終了です： http://chasen.org/~daiti-m/paper/nl190segment.pdf

*1392040581*Re: 東京は住みにくい
タイトルには全く賛同しませんが、大阪の実家から送られてきた白だしは妻に重宝がられています。自宅で食べるうどんがうまくて幸せ（のろけ）

ref. 本の虫: 東京は住みにくい http://cpplover.blogspot.jp/2014/02/blog-post_10.html
</body>
```


[はてなダイアリー 2014-02-10](https://nishiohirokazu.hatenadiary.org/archive/2014/02/10)