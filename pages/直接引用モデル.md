
[[チャットボット]]が会話文を生成するのではなく、単純に書籍などの一節を引用する、というモデル

2018-09-30
- 学習でやろうとしていた
- カギカッコ、中黒、句点などで区切るルールベースに変えた
- 結局のところ「途切れた文章」が生み出されるのが一番頻度の高いバッドパターンなので句点で区切るだけにした
- どちらかというと、この「一文」を入力データとして受け取って、指示語を削るなどして出力する[[seq2seq]]のほうが筋が良いのでは
    - 初期値として素通しにしておく
    - 単語ベース入力、削除する単語の時は空文字列を出力
    - っていうか「素通しするか削除するか」の二値分類がいいのか？

-----
- 引用文自体のモデル
- 引用開始点のモデル
- 引用終了点のモデル

特徴量
- キーフレーズを含む 1/0
- 単語境界である 1/0
- 適度な長さである 適度の定義は何？
- 直前に句点がある 1/0

- 開始点・終了点のモデルはローカルな特徴量だけで計算できるものにしたらいい
    - 周囲の数単語の文脈
- 引用文の長さの適切さは、引用文自体のモデルとして作れば良い
- 引用文自体の良さのモデルはLSTMで作る
- 両方確率モデルにした上で掛け合わせて使えば良い
