---
title: "数学前提知識なしの深層学習"
---

行列の知識を前提としないで深層学習まで最短距離で解説したらどうなるか試しに書いてみた。
この記事ではnumpyとscikit-learnを使う。scikit-learnをインストールすると両方入る
[Installing scikit-learn — scikit-learn 0.21.2 documentation](https://scikit-learn.org/stable/install.html)

### 行列
Numpyを使って解説する。
python

```
>>> import numpy as np
>>> np.array([[1, 2, 3], [4, 5, 6]])
array([[1, 2, 3],
       [4, 5, 6]])
>>> _.shape
(2, 3)
```

コード中の`_`は直前の値という意味。
行列は数値が長方形に並んだもの。上記のコードでは2行3列の行列を作っている。どっちが行でどっちが列かは絶対混乱するので、この解説では今後「(2, 3)の行列」という表現をする。

### ベクトル
「ベクトル」は数が一列に並んだもの。N個の数が並んでいるものを「N次元のベクトル」と呼ぶ。ベクトルはプログラム上はいくつか表現の形がある。
python

```
>>> np.array([1, 2, 3])
array([1, 2, 3])
>>> _.shape
(3,)

>>> np.array([[1, 2, 3]])
array([[1, 2, 3]])
>>> _.shape
(1, 3)

>>> np.array([[1], [2], [3]])
array([[1],
       [2],
       [3]])
>>> _.shape
(3, 1) 
```

2番目と3番目は(1, 3)の行列と(3, 1)の行列なのだが、あまり区別せずにベクトルと呼ばれているケースが多い。

### 行列の掛け算
(A, B)の行列と(B, C)の行列は掛け算をすることができて、(A, C)の行列になる。下では
- (1, 3)と(3, 1)を掛けて(1, 1)になる例
- (3, 1)と(1, 3)を掛けて(3, 3)になる例
を紹介している。
行列の掛け算は整数の掛け算と違って、順序を変えると異なる結果になる。
python

```
>>> v1 = np.array([[1, 2, 3]])
>>> v2 = np.array([[1], [2], [3]])
>>> v1.dot(v2)
array([[14]])
>>> v2.dot(v1)
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])
```


行列の掛け算をする時には、Bの部分の長さが同じであることが必要である。もしずれていると以下のエラーが出る。
下では(1, 3)と(1, 3)を掛けようとして3 != 1だと指摘されている。
python

```
>>> v1.dot(v1)
Traceback (most recent call last):
  File "<ipython-input-...>", line 1, in <module>
    v1.dot(v1)
ValueError: shapes (1,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)
```


### 内積
python

```
>>> v1.dot(v2)
array([[14]])
```

これは 1 * 1 + 2 * 2 * 3 * 3 である。
同じ長さの数値の列があって、対応するものをそれぞれ掛けて全部足している。
これを「内積(Dot product)」と呼ぶ。メソッド名のdotはそこから来ている。

この「3つの対応するものをそれぞれ掛けて足し合わせる」っていうのを数学の書き方をすると
- $x_1 y_1 + x_2 y_2 + x_3 y_3$
になる。これをシグマ(Σ)を使って
- $\sum_{i=1}^3 x_i y_i$
と書く。略して
- $\sum_i x_i y_i$
とか
- $\sum x_i y_i$
とも書く。

### 形式ニューロン
: [形式ニューロン - Wikipedia](https://ja.wikipedia.org/wiki/%E5%BD%A2%E5%BC%8F%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3)
1943年に作られた、のちにニューラルネットの基礎になった考え方
![image](https://gyazo.com/94dd939f71749fa9a2b5070b242ba11a/thumb/1000)

- 入力$x_i$に重み $w_i$ を掛け合わせて足し合わせる
- それが閾値$h$を超えていたら1を出力、超えてなければ0を出力

「入力に重みを掛け合わせて足し合わせる」
- これはさっきの「内積」だ！
- つまり$\sum_i x_i w_i > h$

コードで表現するなら
python

```
>>> if x.dot(w) > h:
...     y = 1
... else:
...     y = 0
```


### ロジスティック回帰
入力$x$と出力$y$の組み合わせをいくつか与えることで、適当な重み $w$を求める方法。
基本的な機械学習の手法の一つ。下のコードでは例えば「入力が(1, 1, 1, 0)なら出力は0」という8件のデータからscikit-learnを使って学習している。角かっこと丸かっこの違いは見やすさ以外の意味はない。
python

```
>>> from sklearn.linear_model import LogisticRegression
>>> X = np.array([
...   (1, 1, 1, 0),
...   (1, 0, 1, 0),
...   (1, 1, 0, 0),
...   (0, 1, 1, 0),
...   (0, 0, 1, 1),
...   (0, 0, 0, 1),
...   (0, 1, 0, 1),
...   (1, 0, 0, 1)])
>>> Y = np.array([0, 0, 0, 0, 1, 1, 1, 1])
>>> m = LogisticRegression()
>>> m.fit(X, Y)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
```


「(1, 1, 1, 0)の場合ってなんだっけ？」って聞くと「0である確率が0.79, 1である確率が0.21」と答える。
python

```
>>> m.predict_proba([(1, 1, 1, 0)])
array([[0.79460161, 0.20539839]])
```


学習データに含まれていなかった(1, 1, 1, 1)の場合にどうなるか聞いてみると、0.55の確率で0だと答える。
確率が0.5に近いのは、あまり自信がないってこと。
python

```
>>> m.predict_proba([(1, 1, 1, 1)])
array([[0.54564474, 0.45435526]])
```


wとhに相当する値はこうやって見ることができる:
python

```
>>> m.coef_
array([[-0.47815958, -0.47815958, -0.47815958,  1.16980067]])
>>> m.intercept_
array([0.08158937])
```


検算してみよう。
説明していなかったが、形式ニューロンとは活性化関数がちょっと違う。[活性化関数 - Wikipedia](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0)
形式ニューロンでは「$\sum_i x_i w_i$が$h$ を超えていたら1、 超えてなかったら0」という階段型のステップ関数を使ったが、ロジスティック回帰では滑らかなS字型のシグモイド関数を使う。
![image](https://gyazo.com/e984c3bc1bedafd04c8e9bf23b8f4410/thumb/1000)
シグモイド関数は数式で書くとこう:
$f(x) = \frac{1}{1 + e^{-x}}$

python

```
>>> m.coef_.dot((1, 1, 1, 1)) + m.intercept_
array([-0.18308872])
>>> 1 / (1 + np.exp(- _[0]))  # シグモイド関数
0.454355256283273
```

ここで出てきた0.45は下記の0.45のことだ。
python

```
>>> m.predict_proba([(1, 1, 1, 1)])
array([[0.54564474, 0.45435526]])
```


### 多層パーセプトロン
ロジスティック回帰は意外な問題が解けない。下記のコードで「0.5」と答えているということは「全然自信がない」ってこと。
python

```
>>>  X = np.array([
...   (1, 1),
...   (1, 0),
...   (0, 1),
...   (0, 0)])
...  Y = np.array([0, 1, 1, 0])
...  m = LogisticRegression()
...  m.fit(X, Y)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> m.predict_proba([(1, 1)])
array([[0.5, 0.5]]) 
```


そこでロジスティック回帰をいくつも積み重ねてみる。
![image](https://gyazo.com/dbe9479365fa1cf7c3f7b65f3e5c0e9c/thumb/1000)

python

```
>>> from sklearn.neural_network import MLPClassifier
>>> m = MLPClassifier(hidden_layer_sizes=[3], max_iter=100000)
>>> m.fit(X, Y)
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=[3], learning_rate='constant',
       learning_rate_init=0.001, max_iter=100000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
>>> m.predict_proba([(1, 1)])
array([[0.92259642, 0.07740358]])
```


「(1, 1)の時の正解は確率0.92で0だ！」と答えれるようになった。めでたしめでたし。
なお、今回はデータが4件とめちゃくちゃ少ないので`max_iter=100000`(最大10万回学習せよ)とめちゃくちゃ繰り返させている。今回は`hidden_layer_sizes=[3]`と小さく薄いネットワークにしているが、ここのパラメータを増やせば広くしたり深くしたりできる。

この学習済みネットワークの重みを見てみよう。
python

```
>>> m.coefs_[0]
array([[ 1.39650888, -2.16802863, -0.96996648],
       [-1.39634665,  2.16818065,  1.11704021]])
>>> _.shape
(2, 3)
>>> m.coefs_[1]
array([[ 2.11163102],
       [ 3.05600029],
       [-1.74200157]])
>>> _.shape
(3, 1)
```


この多層パーセプトロンでは重みが(2, 3)行列と(3, 1)行列の2つになっている。
図を再掲。まず2次元の入力を3つのニューロンにするところで(2, 3)になって、次の3つのニューロンから1つのニューロンにするところで(3, 1)になっているわけだ。
![image](https://gyazo.com/dbe9479365fa1cf7c3f7b65f3e5c0e9c/thumb/1000)

次回予定
- RNN
- LSTM
- Attention

- [https://docs.chainer.org/en/stable/examples/rnn.html](https://docs.chainer.org/en/stable/examples/rnn.html)
    - RNNの説明のところでいきなりLSTMを使っている
