
hatena

```
<body>
*1302662414*TeX用のプリプロセッサを作ってみた
TeXのnewcommand, renewcommandのスコープがどうも納得できなくて、同じことを思った人が過去にいないはずがないのですでにどっかで公開されてるんじゃないかと思ったのだけど、すぐに見つけられなくて作るほうが早そうだったのでプリプロセッサを作ってしまった。

<img src="http://gyazo.com/c8246fa8c2e5af852f5868dda4654ff1.png">
(注: \Gaussと\invは\newcommandされてる)

使ってみて思ったこと
- 文字列のreplaceで置き換えているので\Sとか定義すると\Sigmaの前半にマッチしてしまう。
-- →プログラミング言語だとトークンに刻んであるから短い変数名を使っても「ぴったり一致する短い名前」としかコンフリクトしないから、スコープを限定すれば1文字変数を使ってもまあ実害がない。でも文字列マッチだと短い名前はその文字列を含む名前を全てシャドーしてしまう。きちんとトークンに刻んでから、完全一致のみで置き換えるべき

- 引数を取るマクロが欲しい！
-- →これを文字列マッチなんかでやるとカッコのネストで破滅することが眼に見えているのでちゃんとパースする必要がある

- 今、根元のスコープから順に置換していってるけど、これは適切なの？
-- →置換したものがさらに置換されるケースを考えて、一番近くにあるルールが一番最後に適用されるのが正しいと考えてこの設計にしたが…
--- A: そもそも置換したものをさらに置換するとかすると理解が難しくなるからやめようよ案。変数の解決だって内側から見ていって見つかったらそこで終了じゃないか。(名前の解決だよ派)
--- B: ユースケース「外側で\x→\bm xが定義されていて、内側でphix→\bm \phi(\x)を定義する」を考えると、内側から順に適用するべきだ案。さもないと外側で定義したルールを再利用したルールを作る際に\bm xをもう一度書く必要が出てきてしまう(関数の適用なんだから近い側から順次適用が必要だよ派)
--- !B: それはルール定義のタイミングでルールの本体にその時点で定義されているルールを適用すればいいんだよ派
--- C: (当初の案) ユースケース「外側でEx→\mathbb{E}(x)を定義して、内側の文脈でxが\hat{x}に変わったらx→\hat{x}で『継承＆オーバーライド』したい」を考えて外側から…
--- !C: それは引数を取るマクロで解決すべき問題。
-- Bが正解なのかなぁ

このプロトタイプは1ポモドーロで作れたけど、特に「解決の順番」はちゃんと考えないと破滅を引き起こしそうですな。あとパーサ。そこを考えてしっかりつくり直すと時間かかるかもなぁ。この目的ですでに誰か作ってある程度のユーザに使われてバグが潰されたものがあるなら、これはポイっと捨ててそっちに乗り換えた方がいいと思う。あるんじゃないのかなー。ないのかなー。

<hr>
肝心のソースコードを貼り付けるの忘れてた。

https://gist.github.com/916838

*1302698997*TeX用のパーサを書いてみた
帰りの電車を、確実に座れるところまで反対方向に進んでから帰ってくるって少し遠回りして、パーサを実装してみた。

残念ながらネットワーク接続がなかったので、PLYを入れてみるとかTeXの文法についての形式的な定義を調べるとかできないので気合とフィーリングで適当に。まあテストケースは全部通るように作ったけど、漏れがあるかもねぇ。一応{}によるグルーピングと、それの\でのエスケープまではサポートしてある。

>|python|
"""
texparse.py: parse TeX
"""
from string import ascii_letters
SPACES = " \t"
def spaces(s, i):
    r"""
    >>> spaces("  a", 0)
    ('  ', 2)
    """
    start = i
    while s[i] in SPACES:
        i += 1
    return (s[start:i], i)


def term(s, i):
    r"""
    >>> term("aaa", 0)
    ('aaa', 3)
    >>> term("aaa  ", 0)
    ('aaa', 3)
    >>> term(r"aaa\aaa", 0)
    ('aaa', 3)
    >>> term(r"\aaa aaa", 0)
    ('\\aaa', 4)
    >>> term(r"\{aaa", 0)
    ('{', 2)
    """
    start = i
    escaped = False # T when \ appeared
    while i < len(s):
        if s[i] in SPACES:
            break
        elif s[i] in "{}":
            if escaped:
                return (s[i], i + 1)
            break
        elif s[i] == "\\":
            if i != start:
                break
            escaped = True
        elif s[i] in ascii_letters:
            if escaped: escaped = False
        else:
            if i != start: break
            i += 1
            break

        i += 1
    return (s[start:i], i)


def brace(s, i):
    assert s[i] == "{"
    tokens, i = parse_tokens(s, i + 1)
    return (tokens, i)


def parse_tokens(s, i):
    tokens = []
    while i < len(s):
        if s[i] in SPACES:
            tok, i = spaces(s, i)
            tokens.append(tok)
        elif s[i] == "{":
            tok, i = brace(s, i)
            tokens.append(tok)
        elif s[i] == "}":
            i += 1
            break
        else:
            tok, i = term(s, i)
            tokens.append(tok)
            
    return (tokens, i)


def parse(s, i=0):
    r"""
    >>> parse("hoge")
    ['hoge']
    >>> parse(r"\ho\ge")
    ['\\ho', '\\ge']
    >>> parse(r"  \hoge")
    ['  ', '\\hoge']
    >>> parse(r"a{ho ge}ga")
    ['a', ['ho', ' ', 'ge'], 'ga']
    >>> parse(r"\{aaa\}")
    ['{', 'aaa', '}']
    >>> parse(r"\frac{x^2}{2} + x")
    ['\\frac', ['x', '^', '2'], ['2'], ' ', '+', ' ', 'x']

    """
    tokens, i = parse_tokens(s, 0)
    return tokens

def _test():
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    _test()
||<
</body>
```


[はてなダイアリー 2011-04-13](https://nishiohirokazu.hatenadiary.org/archive/2011/04/13)