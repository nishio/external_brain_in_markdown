---
title: "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws"
---

[2404.05405 Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/abs/2404.05405)

めちゃくちゃ興味深い話
MLP層が100回程度の「新しく知ったこと」を非効率な形で蓄えて、その後1000回ぐらいの間にアテンション層がそれを効率的な形で蓄える形に変わる

> [hillbig](https://twitter.com/hillbig/status/1779640139263901698) LLMはパラメータ一つあたり2bitの情報を様々な後続タスクに利用可能な形で格納できる。他の結果も含めべき乗則の論文以来の重要な結果と思う。人工的に三つ組（名前/属性/値）のデータを設計し様々な規模、アーキテクチャで検証。7Bモデルでもwikipediaと全教科書情報は全て覚えられる。[https://arxiv.org/abs/2404.05405](https://arxiv.org/abs/2404.05405)
>
>  以下いずれも興味深い結果
>
>  この論文では知識を三つ組と単純化して扱っている。
>  wikipediaは45億単語、英語の教科書情報は重複を除いて10万冊分ぐらいあればカバーでき、1冊16万単語とすれば、16億単語ぐらいと考えられる。あわせて200億単語であり、それらに含まれる知識は140億ビットよりは少ないと考えられる、よって2bit/パラメータであれば、70億パラメータあれば十分覚えられる。
>
>  この知識密度は学習中に各知識あたり1000回触れないと（1000-exposure）達成できず、また毎回異なる表現をした方が良いことがその前の論文でわかっている（同じ文章に何回も触れると文章を丸暗記してしまう）。実験では人工的に異なる表現を生成している。
>
>  また100回しか触れないと1bit/パラメータまで記憶可能な容量は落ちる。よって学習中にあまり遭遇しない稀な知識は記憶効率が悪い。
>
>  2bit/パラメータは普遍的であり、記憶の重要要素と思われていたMLPブロックを完全に除いても同じように達成する。これより、自己注意機構がこれまでの想定と違って記憶の役割も果たしていることがわかった。
>  一方100-exposureの場合はMLP層が無いと記憶効率が急激に悪化する。MLP層が少ない知識の露出回数で覚えられるが、記憶容量としてはMLPも自己注意機構も変わらない。
>
>  詳細に実験しているわけではないが最後の層の除く実験ななどから知識は一箇所に格納されているわけではなく、NNの全体に散らばっているとみられる。
>
>  最近の多くのLLMで使われているGated MLPは記憶容量を悪化させ、学習も不安定化させる。他の活性化やトーカナイゼーションの違いは記憶容量には関係ない（GPT-2でもrotary埋め込みを使っている限りは最新のアーキテクチャと変わらずGated MLPを使ったモデルより優れている）。今後のNN設計に影響を与える結果。
>
>  量子化はint8までは記憶容量に全く影響がでない。int4から記憶容量が急激に悪化する。別の言い方をすれば2bitの情報をNN内で8bit（int8）で格納すれば後は自由自在に使えるような形になっており、NNを後続タスクで自由に使うための索引と考えた場合、索引オーバーヘッドの限界が4倍。学習中から量子化を考慮していればもっと可能かもしれない。
>
>  MoEで32エキスパートを使った場合、推論時に8.8%しかパラメータを使わないが、容量は1.3倍悪化、100-exposureは1.5倍悪化するだけで抑えられる。
>
>  また、学習データに質の悪いデータが含まれている場合、記憶容量に大きな影響があり、100-exposureの場合20倍も記憶容量が悪化する。
>
>  これを解決する工夫は簡単で質の良いデータ（wikipediaや教科書など）の前に、特殊トークンを付けて質の良いデータとLLMに教えておくだけで記憶容量はほぼ最適に改善される。
>
>  == 以下感想 ==
>  現実世界のデータの場合は同じ知識が何回露出しているかはべき乗則のようになっており、有名なものは100~1000回以上出現しているが（日本の首都は東京とか）大部分の一部しかしらない分布の末端の知識は100回よりもずっと少なく、10回とか1回とかがほとんどと思う。現在のLLM学習では1~2epochである。
>
>  今後はNNを少ない回数の露出で記憶できる工夫をするか、知識を濃縮して再生成して覚えさせるかが必要。
>
>  また今回の話はTransformerの場合の限界であり、これがもっと普遍的な話かはわからない。一方様々なアーキテクチャで全く同じ結果なので何か理論的な背景があるかもしれない。
>
>  小さなLLMの場合は良質なデータに加えて人工的にきれいなデータを作り、さらに重要な知識は元データで出現回数が少ないものは何回も露出（言い換えて）させることで覚えさせることで記憶できる。
>
>  例えば、論文中の(2.1）の例は75トークンで7つの知識（三つ組）が含まれており、知識一つあたり10トークンぐらいで表される。一つの知識を1000回露出するには1万トークンあればよいことになる。学習データ1兆トークンあれば、1億個の知識を1000回露出できることになる
>
>  なお、本論文では三つ組で表される知識についての評価で他の形の知識や、推論能力など別の話。

