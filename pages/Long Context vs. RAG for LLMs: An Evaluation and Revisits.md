---
title: "Long Context vs. RAG for LLMs: An Evaluation and Revisits"
---

[https://arxiv.org/abs/2501.01880](https://arxiv.org/abs/2501.01880)
<img src='https://scrapbox.io/api/pages/nishio/o1 Pro/icon' alt='o1 Pro.icon' height="19.5"/>
論文の前半では、LCやRAGを扱った複数の研究を体系的にサーベイし、それぞれの研究がどんな結論や主張をしているかを一覧表などでまとめています。たとえば、
- LCがRAGより優れている：すべての文書を一括で入力できるので、特に物語・長文読解などの「まとまった文脈」が有効な場合に強いという主張。
- RAGがLCより優れている：要点だけを抜き出すため、雑多なノイズの多い長文でも性能が下がりにくく、さらにはモデルの計算コスト（トークン数）を節約できるという主張。
- 両者を組み合わせる：たとえばロングコンテキストモデルでも「必要に応じてさらにretrieverを使う」といったハイブリッド手法が良い、と結論づける研究もある。
しかし、先行研究間で（1）使用しているモデルやパラメータサイズ、（2）使うデータセットの種類や文書長、（3）評価方法の違いなどによって、結論が食い違っていることも指摘されています。


3. 評価データセットの作成と手法
- 3.1 質問のフィルタリング
    - 多くの研究で用いられるQAデータセットには、元々LLMのパラメトリック知識だけで答えられるものも混ざっています。そうした「外部文書を読まなくても答えられる」質問が紛れ込むと、RAGやLCによる性能向上を純粋に比較できないおそれがあります。
    - そこで著者らは、大規模言語モデル（GPT-4の一種）に質問を投げて、外部コンテキスト無しに正解してしまう質問は排除しました。こうして「どうしても外部文書が必要な質問」だけを残すことで、公平な比較ができるようにしています。
- 3.2 質問の拡張
    - 既存のデータセットは質問数が少なく、100〜200問程度のものも多いです。この程度では統計的に十分な比較を行うのが難しいため、著者らは以下の方法でデータを拡張し、約2万問以上の大規模なQAセットを構築しました。
    - 元のデータソース（Wikipediaや小説など）から追加の質問や文脈（ダミー文章も含む）を収集。
    - データセットによっては合成的に文書を長くし、長文を与えた際のLLM挙動をテストできるように加工。
- 3.3 実験に使う代表的なデータセット
    - 論文では以下のような12種類のQAデータセットが採用されます。Wikipediaベースの知識系や、複数文書を扱うマルチホップ推論系、長い物語や論文からの読解系など、バリエーション豊かです。

4. 評価フレームワーク
- 著者らの評価は以下の三段階に分かれています。
    - Phase 1: Retriever（検索モジュール）の比較実験
        - 代表的な検索手法としてBM25（スパース）、Contriever（密ベクトル）、OpenAI Embeddings、Llama-Index（Index型）、RAPTOR（要約型）などを比較し、どのretrieverが最も有力か調査。
        - 実験では「[[RAPTOR]]」が総合的に最も高い性能を示すと結論付けています。
            - 要約ベースのRAPTORが最も優れた正解率 (38.5% 等) を示し、Llama-Index系の木構造アプローチがそれに続いた。BM25などの伝統的（または単純な）チャンク分割型は長文の中に散らばった情報を拾いきれず、やや劣った。
    - Phase 2: RAG vs. LCの性能比較
        - Phase 1で選んだ最良のretriever（RAPTOR）を用い、RAGパイプラインとLCモデル（文書をすべて一括入力するモデル）を大規模QAセットで比較。
        - 全体的には、LCのほうが正答率が高い（約56% vs 49%）という結果。
            - ただし、全体の10%程度は「RAGのみが正解にたどり着く」ケースもあり、RAG特有の強みが見られるシナリオが確かに存在。
            - データセット別には、たとえば物語やWikipediaに由来する長文など「1つの文脈が大きめにまとまっている」分野ではLCが強い。
            - 一方、複数の断片的対話ログや断片的ドキュメントを扱うようなデータセットではRAGが優位を示すこともあった。
    - Phase 3 (ケース分析)
        - LCが失敗してRAGが正解できるケースの多くは、「文脈が会話形式などで断片的であり、RAGで局所的に抽出したほうが正確に答えを拾える」というパターン。
        - RAGが失敗してLCが正解するケースは、「質問に関連する情報が複数段落にまたがる」「RAGのチャンク分割で情報が途切れがち」「retrieverが正しくピックアップできなかった」という例が多い。
        - 文書タイプ・質問タイプ（”Who”や”Where”などの事実系、あるいは”How”や”Why”のような説明系）ごとに性能を可視化すると、やはり「読み物や長文を一挙に読み込む必要のある質問」ではLCが強く、「散在情報を検索してまとめる必要のある質問」や「Yes/No型などシンプルな応答」ではRAGが勝ちやすい、という傾向が明確になった。

[[Long Context]] vs. [[RAG]]
