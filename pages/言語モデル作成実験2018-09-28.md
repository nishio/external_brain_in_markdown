
![image](https://gyazo.com/6c848c9690a3289e512c09cb3f72cc35/thumb/1000)

エンジニアの知的生産術の本文部分を[[LSTM]]で学習 #LSTM-RNN
:

```
%run train_rnnlm.py
#vocab = 7069
epoch       iteration   perplexity  val_perplexity
3           500         207.066     168.164         
7           1000        55.6905     88.8363         
11          1500        32.3283     83.8262         
15          2000        19.5342     96.141          
19          2500        12.4958     115.902         
23          3000        9.15305     138.699         
26          3500        6.81103     159.106         
30          4000        5.71608     178.38          
34          4500        5.07816     201.215         
38          5000        4.33354     223.951         
test
test perplexity: 333.32472655247705
```

15エポックで止めても良かったのでは無いかって気はするな
- →2000イテレーションぐらいの時のモデルで文章生成してみたけど、主観的には最後のモデルの方が良いように思う

LSTM言語モデルを使った文章生成実験:
- > 	知的の大きい方向・Realistic :タプルの写真を比較して再度、繰り返しのサイクルを回す
- >  	エンジニアの壁7つの意識に出てきていないせい❹アイデアが新しいアイデアを思い付く
- >  	成長することについては抽象化のことを思い出し、理解は8時間頻繁に「これは関係が
- >  	理解することです。哲学言語にはUC Vです。片方ではなく、目次は裏
- >  	繰り返し読みます。立て続けにはまず考えましょう。たとえば木Mihaly (エジソン)は、登山
- >  	なぜならJames Webb Youngのアクセスを見ています。私が同感で、「U理論」
- >  	エジソンは、保守やエンジニアリングの間に)❸ように編集するたびのように指示すること
- >  	妻の達人研究探検は、状態を積極的に使います。損益分岐点は、この設計を過ぎ

局所的な単語のつながりはかなりスムーズだが、まあ文章全体としては無意味だな。

上記の出力は単語を1つ与えて、その後20単語を生成しているが、しばらく走った方が良い雰囲気の文章を出力することが多いように思う

- > 妻の達人研究探検は、状態を積極的に使います。損益分岐点は、この設計を過ぎた商品のうちに「関係がありそう」と答えたときに「こんなに目次が必要だと思ったゲームを進めることはできない」という意図で止まっているわけです。たとえばこのなんとなく関係は15にくいのが語源です。一方で、物理的な先生に能動的に学ば行くものではなく、手を動かしながら読むのはやめることの方法です。二兎を追う者は一兎をも得ず、という感想が湧いてきたときに区別できないことが大事です。話題がつながるようになることを、まずは経っていたタスクが記録しても、うまく当たりがあなたの経験ためには多少のURLからダウンロードできます。
- > 知的の大きい方向・Realistic :タプルの写真を比較して再度、繰り返しのサイクルを回すことで、理解が検証します。実験検証は、仮説を連想するいくつか的なものだったのかもしれませんね。自分が自分で選ばれる❶を経験することは、目的を明確化する必要があります。私には探索と気付くことがあります。世界はジンベイザメや楽しさ、他者の概念を手放すことで、効率良く記憶を作ることです。今質問を作るタスクがいっぱいで混乱すると、混乱で新しい知識を作る必要があります。たとえば「アイデアを思い付く」はあいまいで大きなタスク芽生える、「自分の言葉でできるのは間違っているのだろう」という悩みも実感されます。

これは、学習時はたくさんの単語を連続して入力されているので、1単語の入力で作られる内部状態は学習時の内部状態の分布から離れたところにあるせいだろう。[[文章生成は複数単語を入力として受け取る]]
しばらく走った後の単語列の中から、適当な開始地点と終了地点を見つけ出して切り出すモデルは、原文からの[[直接引用モデル]]と同じ仕組みで良さそう。

時間に関する話題を長めに続けたりする。文脈を長く保持できるのはLSTMならでは。
- > 対策や、145分、隙間時間をかけていくという意味です注6。「大きな時間を確保しない」は複数のタスクのサイズを約わかり、最適な落としに気付かずに悪い替わりを読み進めて、GTDの中の執筆に基づいた情報から時間を感じ取れていない状態状態Sensing :他者の経験に基づいて一番期待値の高い選択肢を選ぶ、というアルゴリズムで動くようにしました。逆に、振り返りも同じように、自分の残像とCowanのギャップに枠が入っている人にが使われているということを追う並べて「見つけてきた」と考えるとき、その「全体像を把握」に「抽象」という言葉のような部分です。日々この訓練をもとに、どういう知識を想定するつもりを作るのか、なかなか観察に置くことでしょう。読む速度のピラミッドではやさしいでしょうか?たとえば「大まかに全体像を把握」という

これは見出しか何かの「第1章新しいことを学ぶには 情報収集の3つの方法」を拾ってしまっているのだろう。同様に脚注を拾ってしまっていて変な感じになっているところもある。
- > 私は50mm ×38mmのふせんを好んで行う第1章新しいことを学ぶには、サイクルを回す原動力:やる気8第1章新しいことを学ぶには情報収集の3つの方法割り当てる方法は、会話をめくる箇所を継続したいとしてとして、1964年者を学ばすることがあります。これはたった1ページの30秒という「1冊30秒」という言葉が生まれ、2時間が終わる

元データの一部
> ...しかし、そうではありません。まず最優先で卓越し、そ第7章何を学ぶかを決めるには自分経営戦略234
> 第7章何を学ぶかを決めるには自分経営戦略235れによって成長の機会を得るのです注15。...
あー、なるほど。そうなっているのか。
これを取り除く必要がある: [[ページの行ベース言語モデル]]
