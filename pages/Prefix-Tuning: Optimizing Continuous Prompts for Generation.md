
> Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.
[https://arxiv.org/abs/2101.00190](https://arxiv.org/abs/2101.00190)
(DeepL)ファインチューニングは、事前に訓練された大規模な言語モデルを活用して、下流のタスクを実行するための事実上の方法である。しかし、ファインチューニングはすべての言語モデルのパラメータを変更するため、タスクごとに完全なコピーを保存する必要がある。本論文では、自然言語生成タスクのためのファインチューニングに代わる軽量な方法として、言語モデルパラメータを凍結したまま、タスク固有の小さな連続ベクトル（プレフィックスと呼ばれる）を最適化するプレフィックスチューニングを提案する。プレフィックスチューニングはプロンプトからヒントを得ており、後続のトークンがあたかも「仮想トークン」であるかのように、このプレフィックスに注目することを可能にする。我々は接頭辞チューニングを、表からテキストへの生成のためのGPT-2と要約のためのBARTに適用する。パラメータの0.1%のみを学習することで、prefix-tuningは全データ設定において同等の性能を獲得し、低データ設定においてfine-tuningを凌駕し、学習中に未見のトピックを含む例に対してより良く外挿されることがわかった。
