---
title: "Hatena2014-02-11"
---

hatena

```
<body>
*1392045080*論文紹介「ベイズ階層言語モデルによる教師なし形態素解析」
形態素解析しなくても単語に分割できる、しかも教師データがいらないので古文や未知の言語でもOK、という論文。

Deep Learningの勉強をアウトプットしながらやるために始めた「Deep Learning論文紹介」企画だけども、いきなりDeep Learningではない論文になってしまったのでタイトルからDeep Learningを削りました。文脈は陸続きなのだけどね。

まず階層的Pitman-Yor言語モデル(HPYLM)の説明。

Pitman-Yorはある確率分布「基底測度」を元に、似た確率分布を生成する確率過程。バイグラムの分布はユニグラムに似てるし、トライグラムの分布はバイグラムに似ているので、階層的にPitman-Yorを積み重ねたらいいんじゃないの、という話。

実装としては中華料理店過程(CRP)で実現。Trie木みたいに中華料理店のテーブルが連なっていて、お客さんが末端のテーブルに追加されたり、テーブル自体を作ったりする。お客さんは上のノードに複製されて伝搬する。お客さんが追加されるだけだと単に数えているだけなのでお客さんが消えたりもするんだと思う。(追記：と思ったが消えたりはしない？復習が必要）(追記：CRP自体は客は消えないけどギブスサンプリングのために消して足すことを繰り返すんだと思う)

学習にはMCMCを使う。dとθの推定は別の論文。

で、ここまではHPYLMの話。単語HPYLMの一番上のゼログラム頻度はどうやって決めるか？全単語が等確率で出る？いやいや、そこに文字HPYLMを使いましょう、というのがこの論文の提案するNested PYLM(NPYLM)

さて単語分割をどうやってサンプリングするのか。1箇所選んで、そこで区切られるかどうか分布からサンプリングして…というギブスサンプリングで可能だが、遅いし収束しないしで辛い。そこでBlocked Gibbs Sampler。要するに文章全体を一気に単語分割する。

一気に分割するっていうけど具体的にはどうするの？「文末からk個が単語をなす確率」のテーブルを作る。これは再帰的に定義されているのでDPで計算できる。次にそのテーブルを使って文末から単語分割をサンプリング。

ただ、これだと長い単語の出現頻度が小さくなってしまうのでポアソン分布になるように補正する。長さkの単語が出る確率をモンテカルロで決めてそれで割り、ポアソン分布を掛けてやる。ポアソン分布のパラメータは共役事前分布のガンマ分布で決めて、ベイズ的に更新する。

結果：色々紹介されているけど、空白を潰してくっつけたAlice in Wonderlandがだいたい正しく分割されているから凄い。-lyや三単現のsなんかが分離されているのが人間の考える「単語」とはちょっとずれているけど、その方が言語モデルとしては適切なのかもしれない。実際、教師あり・半教師ありでも学習させられるけども、教師なしのほうが言語モデルとして適切らしい。

感想：まずは文字HPYLMを実装してみて理解のあやふやな点を探すかな。あと次回は「階層 Pitman-Yor 過程に基つ&#12441;く可変長 n-gram 言語モテ&#12441;ル」の紹介かな。

** 出典
http://chasen.org/~daiti-m/paper/nl190segment.pdf

** さいごに
この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。
</body>
```


[はてなダイアリー 2014-02-11](https://nishiohirokazu.hatenadiary.org/archive/2014/02/11)