---
title: "ウェブサイトをミラーリング"
---

<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>wgetでサイトをダウンロードする際に /fooと/foo/barがあり、/fooでファイルが作られてしまうため/foo/barを保存することができずにエラーになる。何か回避する方法があるか？

<img src='https://scrapbox.io/api/pages/nishio/gpt/icon' alt='gpt.icon' height="19.5"/>wgetだけで直接この問題を解決する機能は限られています。代わりに、問題を回避するためにはいくつかの異なるアプローチが考えられます。
- 別のツールを利用する: wget以外にも[[ウェブサイトをミラーリング]]するためのツールがあります。httrackやcurlを使用する方法があります。これらのツールは異なる方法でファイルを処理する可能性があるため、問題を回避できるかもしれません。
- スクリプトを使用する: wgetを使ってファイルをダウンロードする前に、URLリストを生成し、問題のあるファイルやディレクトリを特定して、それらを個別に処理するスクリプトを書くことができます。...

<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>curlならどうなる？

<img src='https://scrapbox.io/api/pages/nishio/gpt/icon' alt='gpt.icon' height="19.5"/>curlを使用する場合、wgetとは異なり、主に単一のHTTPリクエストを実行し、その結果を標準出力に出力するか、指定したファイルに保存します。デフォルトでは、curlはウェブサイトの構造をミラーリングしたり、複数のリソースを再帰的にダウンロードしたりする機能を持っていません。そのため、同名のファイルとディレクトリが存在する問題を直接扱う機能もありません。

<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>httrackをMacで使う方法を教えて

<img src='https://scrapbox.io/api/pages/nishio/gpt/icon' alt='gpt.icon' height="19.5"/>...
`$ brew install httrack`
...

---
[[httrack]]をbrewで入れて使うことにした<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>
- あっさり目的が果たせた

「最近Google使わなくなった」「Googleだって自然文で検索できますよ」という話をしたのでGoogleでも同じ入力をしてみたが、httrackを教えてはくれなかった(検索結果は全部wget)、下記は[[SGE]]の結果
![image](https://gyazo.com/1e3a81034430cb0c93636612225f72e4/thumb/1000)
