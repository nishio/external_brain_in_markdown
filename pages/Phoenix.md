---
title: "Phoenix"
---

この記事は、大規模言語モデル（LLM）の評価方法について解説しています。要点は以下の通りです。
- LLMを使ったアプリケーションが増えるにつれ、LLMのパフォーマンス測定が重要になっているが、これは簡単ではない
- LLMモデル自体の評価（モデル評価）とLLMを使ったシステムの評価（システム評価）は区別すべき
- システム評価では、プロンプトテンプレートやコンテキストなど、開発者が制御できる部分の評価が中心
- 評価指標はユースケースによって異なる（情報抽出、質問応答、RAG等）
- 評価にはLLM自体を使うのが効果的。正解データを使ってLLM評価の精度をベンチマークし、それを本番システムに適用する
    1. まず評価用の「ゴールデンデータセット」を用意する。これは評価で期待されるデータの種類を代表するもので、人手でラベル付けされた正解ラベルを含む。
    2. 次に評価用LLMを選ぶ。これは評価対象のシステムで使われているLLMとは別のものでもよい。コストと精度のバランスを考慮する。
    3. 評価用プロンプトテンプレートを作成する。入力の構造（検索されたドキュメント、ユーザークエリなど）、LLMに何をさせるか（ドキュメントの関連性判断など）、期待する出力フォーマット（関連/無関連の2値など）を明確にする。
    4. ゴールデンデータセットに対して評価を実行し、正解率、適合率、再現率などの指標を算出してベンチマークとする。プロンプトテンプレートを改良して精度を上げるのは反復的なプロセスになる。
    5. 最適化されたプロンプトテンプレートでのゴールデンデータセットに対する性能が、評価の信頼性の指標となる。人手より精度は落ちるが、大量のデータを安価に評価できる。
- 評価指標は全体の正解率だけでは不十分で、適合率や再現率を見るべき
- 評価は開発前、本番前、本番後の各タイミングで実施する
- どのモデルを評価に使うかはユースケース次第だが、トレードオフを理解する必要がある
    - コストと精度のバランス。大規模で性能のいいモデルほど評価の質は上がるが、コストもかかる。
    - 再現率と適合率のバランス。ユースケースによってどちらを重視するかは異なる。
    - 評価用と本番用のモデルをどの程度一致させるか。モデルの特性を一致させたほうが評価の確度は上がるが、汎用的な評価用モデルのほうが使い回しがきく。

全体として、LLMシステムの評価は重要だが難しい課題であり、ベストプラクティスやツール、データセットの整備が進んでいるという内容でした。
