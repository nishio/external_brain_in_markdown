
> Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for [[retrieval-augmented generation]] ([[RAG]]) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.
>  ![image](https://gyazo.com/efa1e9860131e5d996467f4f197d6959/thumb/1000)
[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)

(DeepL) 事前に訓練された大規模な言語モデルは、そのパラメータに事実知識を格納することが示されており、下流の自然言語処理タスクで微調整を行うと、最先端の結果を達成する。しかし、知識にアクセスして正確に操作する能力はまだ限定的であるため、知識集約的なタスクでは、タスク固有のアーキテクチャに比べて性能が劣る。さらに、その決定に実証性を与え、世界知識を更新することは、依然として未解決の研究課題である。明示的なノンパラメトリックメモリへの微分可能なアクセス機構を持つ事前訓練されたモデルは、この問題を克服することができるが、これまでのところ、抽出的な下流タスクについてのみ研究されている。本研究では、言語生成のために事前に訓練されたパラメトリック記憶とノンパラメトリック記憶を組み合わせたモデルである、検索補強型生成（[[RAG]]）のための汎用的な微調整レシピを探求する。我々は、パラメトリックメモリが事前に訓練されたseq2seqモデルであり、ノンパラメトリックメモリが事前に訓練されたニューラルレトリバーでアクセスされるウィキペディアの密なベクトルインデックスであるRAGモデルを紹介する。RAGの2つの定式化を比較する。1つは生成されたシーケンス全体にわたって同じ検索された文章を条件とし、もう1つはトークンごとに異なる文章を使用することができる。また、3つのオープンドメインのQAタスクにおいて、パラメトリックseq2seqモデルやタスクに特化したretrieve-and-extractアーキテクチャを凌駕し、最先端を行く。言語生成タスクにおいて、RAGモデルは、最先端のパラメトリックseq2seqのみのベースラインよりも、より具体的で、多様で、事実に基づいた言語を生成することがわかった。

関連
- [[検索を組み合わせる]]
