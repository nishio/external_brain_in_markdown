---
title: "Hatena2011-11-08"
---

hatena

```
<body>
*1320717651*numpyのinvertは逆行列ではない
おっと。invertで逆行列をつくろうとしたら謎の行列が返ってきた。
>||
In [1]: invert(array([[4, 0], [0, 1]]))
Out[1]:
array([[-5, -1],
       [-1, -2]]) 
||<

invertは「Compute bit-wise inversion, or bit-wise NOT, element-wise.」だそうな。invを使うのが正解。(thanks id:n_shuyo)
>||
In [2]: inv(array([[4, 0], [0, 1]]))
Out[2]: 
array([[ 0.25,  0.  ],
       [ 0.  ,  1.  ]])
||<

invはlinalgの中で定義されている。
>||
In [3]: inv
Out[3]: <function numpy.linalg.linalg.inv>

In [4]: invert
Out[4]: <ufunc 'invert'>
||<

*1320724692*Numpyで混合ガウス分布のEMアルゴリズムを実装した
[f:id:nishiohirokazu:20111108125655g:image]
楕円は3標準偏差、初期値からEMステップを1回、2回、3回、7回、15回(平均のベクトルの変化量が0.0001未満という基準で収束と判定)の6枚でアニメーションGIFにしてみた。[f:id:nishiohirokazu:20111108143113g:image]
こっちはEMステップを0～15回実行。k平均法ではこれはできまい。しかし初期値によっては期待と違う収束の仕方をするのも観測できる…と言いながら違う収束の仕方をする初期値を探したけどうまく行かなかった(苦笑)
[f:id:nishiohirokazu:20111108144600g:image]
コレも時間はかかっているけどもうしばらく学習を続ければ、内側のクラスタが片方の斜め線を殆ど食べてしまって、外側のクラスタのつっかえ棒が無くなってしぼんできて、最終的にやっぱりXになりそうだ。
[f:id:nishiohirokazu:20111108144943g:image]

*1320724994*ImageMagickで動画GIFを作成する方法
>||
$ convert -delay 100 -loop 0 [1-6].png anime.gif
||<

楽チン。

>||
$ convert -delay 100 -loop 0 fig*.png anime.gif
||<

10枚を超えてきたら"%02d"とかでfig00.png ... fig99.pngにしてアスタリスク(*)で取ったほうがよさげ。

*1320740584*Jubatusワークショップに行って来ました
昨日行われた<a href='http://www.zusaar.com/event/165003'>Jubatus Workshop</a>に参加してきました。ref. <a href='http://togetter.com/li/211053'>第一回 Jubatus Workshop #jubatus - Togetter</a> <a href='http://www.slideshare.net/JubatusOfficial'>JubatusOfficial Presentations</a>

以下は僕の解釈も含めたまとめ
- ユバタスの必要性
-- データはこれからも増加していく。今多いってことより、今後どんどん増えるということが重要な問題
-- データの活用は　蓄積→理解→予測。世の中はようやく蓄積が出来るようになり理解に向かい始めた段階. 
-- CPUの速度が向上する速度よりも、データ量の増える速度の方が大きい。必然的にデータに対する処理で並列性を考えなければいけない状況が増えてくる。

- 既存のシステムとの差
-- 既存のシステムでは 1. リアルタイム性の確保 2. データの水平分散処理 3. 高度な解析 の3つを兼ね備えることが困難。
-- MapReduceは処理の自由度は高いが基本的にバッチであり解析結果はすぐには帰ってこない, CEPなどオンライン/ストリーム処理は単純な処理しかサポートしてないものが多い, 
-- Hadoopとは異なる方向性を模索しようとしたのがユバタス
-- MIXが肝

- 「全ての人に機械学習を」
-- 特徴ベクトルの抽出などを設定ファイルで指定して組み合わせる仕組み
-- プラグインで拡張もできる

- MIX
-- モデルパラメータの共有をリアルタイムにしないという妥協によって実現可能。モデルパラメータの間欠的なMIXでOKだというのは論文にもなっている
-- 現状は線形分類器の並列化になっていて、モデルパラメータのMIX処理("Iterative Parameter Mixuture")は単なる平均。
-- unnonouno「Iterative Parameter Mixtureについてはこの辺を参照 http://t.co/ApubeIxC」hillbig「平均だと大分乱暴な気がしますが、パーセプトロンぐらいだったら、収束性能が示されています。 http://t.co/r384e8AH」

- ユースケース：パケットデータからの電力消費量推定
-- 回帰がまだ実装されていないので教師あり学習を使った、という柔軟な発想
-- ARPパケットを捨てるとパケット量で電力推定がそれなりに実用的な精度でできた。
-- この結果を出すためのプログラムがPythonのスクリプト1個, 100行くらいでできていて, 半分が元データの整形で使われている. 

- pficommon
-- C++でfrom_jsonとかto_jsonとか
-- C++でサクッとRPC
-- MPRPCにすればMessagePackになる

- テキスト寄り？
-- sla「ベクトルがKey-Valueだよ、っていうのは一般化としてはそうだけど、やはりテキストに寄った実装なのかな～」
-- hillbig「非定形データ（bag of words, bag of image feature, 行動履歴）の特徴ベクトルが疎ベクトルなので、それ向けの実装になっています。元々テキストで実証実験を進めたのでテキスト向け機能が充実していますが、他も考える予定です。」

- 学習差分だけの通信で通信量が減る理由
-- nishio「差分を通信したらモデル全体を通信するのより楽, というのがよくわからないなぁ. たとえば線形分類器だと各クラスタ×各特徴×floatとかで重みベクトルの塊があるんだと思うが, それの前の学習からの変化量ってやっぱり同じ個数のfloatじゃないの？ *YF*」
-- niam「重みベクトルの非0の部分だけ通信しているのかな？だったらモデル全体ではなく差分だけ通信することで通信量下がるのはちょっとわかる気が。」
-- nishio「自然言語を想定した場合, モデルパラメータの差分はスパースなので非0だけ送れば圧縮できる, というところまでは理解できた.」
-- つまり、hillbig「非定形データ（bag of words, bag of image feature, 行動履歴）の特徴ベクトルが疎ベクトルなので、それ向けの実装になっています」ということね

その他
- Twitterの内容を多クラス分類するのが6000QPSで走るらしい！(詳しい情報どこ？)
- Twitterのストリームを処理するくらいならマシンは2台程度で十分らしい(詳しい情報どこ？)
- 教師あり学習だけではなく, 1月くらいをめどに近傍探索と属性補完(レコメンドの要素技術)にもチャレンジとな. ストリーム化はアカデミック的にもチャレンジング. 将来的には統計分析やHMM,LDAなんかも乗せると。
- 入力データがBag of wordsやn-gramでベクトル化され, 線形分類のオンライン学習に流し込まれる. 中の仕組みは今回は詳しいことは言わないらしい. パーセプトロンとPA, CW, AROW, <del>NHEAD</del>NHERDの5つが既に実装済み。
- MIX処理の開始時にはロックを獲得するが, これは他の学習機がMIXを開始しないためのもので, MIX処理中でも個々が差分を送ったり受け取ったりしている間以外は学習や分類を受け付けられる,ただしMIX処理中の学習結果は捨てられる,これは不可避とのこと
- hillbig「データではなくモデルだけをやりとりするので、ネットワークがネックにならないというのが売りではあります。将来的にネットワークバンド幅が細い、組み込み向けでの利用もできたらなと考えています」
- jubakeeperがやってくれるので、1台か複数台かを考える必要がない
</body>
```


[はてなダイアリー 2011-11-08](https://nishiohirokazu.hatenadiary.org/archive/2011/11/08)