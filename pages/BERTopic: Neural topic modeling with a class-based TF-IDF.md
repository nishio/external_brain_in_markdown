---
title: "BERTopic: Neural topic modeling with a class-based TF-IDF"
---

[2203.05794 BERTopic: Neural topic modeling with a class-based TF-IDF procedure](https://arxiv.org/abs/2203.05794)
[[BERTopic]]: Neural [[topic model]]ing with a [[class-based TF-IDF]] (2022)

<img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>要約して/クラスベースTF-IDFとは？/LDAなどに比べたメリットは？/HDBSCANは何に使うの？/UMAPは何に使うの？どのような指標によってこの手法が優れていると主張している？/CTMって何だっけ/上記のやりとりを、僕の質問文は温存しつつ、ノートに保存するための要約したやりとりにして

<img src='https://scrapbox.io/api/pages/nishio/claude/icon' alt='claude.icon' height="19.5"/>BERTopicは、事前学習済み言語モデルとクラスベースTF-IDFを用いた新しいトピックモデルです。従来のLDAなどと比べ、以下のような利点があります。
- 言語モデルを活用し、単語の文脈を考慮したトピックモデリングが可能
- ドキュメントのクラスタリングとトピック表現の生成を分離し、柔軟なアーキテクチャを実現
- 動的トピックモデリングへの応用が容易
- 異なる言語モデルでも安定した性能を示す

BERTopicでは、まずドキュメントを言語モデルで高次元ベクトルに埋め込みます。次にUMAPを用いて次元削減し、HDBSCANでクラスタリングすることでトピックを抽出します。最後にクラスベースTF-IDFでトピック表現を生成します。

[[クラスベースTF-IDF]]は、トピックごとにドキュメントをグループ化し、そのグループ内での単語の重要度を計算する手法です。これにより、トピックを特徴づける単語をより直接的に抽出できます。

[[UMAP]]は非線形次元削減手法で、高次元データの局所的・大域的構造を保持しつつ、効率的に低次元空間に埋め込むことができます。[[HDBSCAN]]は密度ベースのクラスタリング手法で、クラスタ数を指定せずに、ノイズに頑健なクラスタリングを実現します。

BERTopicは、トピック一貫性とトピック多様性の指標で優れた性能を示しており、CTMなどの他の最新手法と比べても、計算効率の面で優位性があります。ただし、1つのドキュメントが複数のトピックを持つことを直接モデル化できないなど、課題も残されています。

[[CTM]]は、BERTopicと同じく言語モデルを活用した最新のトピックモデルです。[[VAE]]ベースのアプローチを取っており、単語の文脈情報と出現頻度を統合して、end-to-endでトピックを学習します。トピック品質では優れた結果を示していますが、計算コストが高いという問題があります。

[[クラスベースTF-IDF]]
---
評価指標のところは要約しすぎ

<img src='https://scrapbox.io/api/pages/nishio/claude/icon' alt='claude.icon' height="19.5"/>
BERTopicの優位性は、主に以下の2つの指標によって評価・主張されています。

1. トピック一貫性（Topic Coherence）
トピック一貫性は、トピックを構成する単語がどの程度意味的にまとまっているかを測る指標です。具体的には、トピック内の単語間の意味的な類似度や共起確率などに基づいて計算されます。

論文では、正規化ポイントワイズ相互情報量（Normalized Pointwise Mutual Information, NPMI）というトピック一貫性の指標が用いられています。NPMIは、トピック内の単語ペアの共起確率と、それぞれの単語の出現確率から計算されます。

実験結果では、BERTopicがLDAやNMFなどの従来手法と比べて、高いNPMIスコアを達成しています。これは、BERTopicが意味的にまとまりのあるトピックを生成できることを示唆しています。

2. トピック多様性（Topic Diversity）
トピック多様性は、生成されたトピック間の重複度合いを測る指標です。トピック間で単語の重複が少ないほど、トピック多様性が高いとみなされます。

論文では、トピック多様性を、全トピックにおけるユニークな単語の割合として定義しています。0に近いほどトピックの重複が多く、1に近いほどトピックが多様であることを示します。

実験結果では、BERTopicが比較的高いトピック多様性を達成していますが、CTMには及ばないことが報告されています。ただし、BERTopicはCTMと比べて、言語モデルの選択による性能の安定性が高いことが示されています。

これらの指標に加えて、論文ではBERTopicの計算効率についても議論されています。BERTopicは、CTMと比べて格段に高速であり、大規模データセットに対しても現実的な時間で処理できることが示されています。

以上のように、BERTopicは、トピック一貫性、トピック多様性、計算効率の観点から、従来のトピックモデルと比べて優れた性能を示しています。ただし、著者らも認めているように、これらの自動評価指標はあくまで近似的なものであり、実際のユーザーによる主観的な評価も重要であることに注意が必要です。


---
[BERTopic](https://maartengr.github.io/BERTopic/index.html#modularity)
[[CountVectorizer]]

[[トピックのクラスタリング]]
