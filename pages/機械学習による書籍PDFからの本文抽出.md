
ここまでの流れ  #書籍スキャンPDF
- 書籍のテキスト化したものには、ページ番号や脚注など色々な文字列が含まれている
    - 当初[[行継続判定]]で、テキストを直接つなぐか空白を挟んでつなぐか識別すれば良いと考えていたが間違い
- 1ページの情報に対して、行の単位で「これは何か」を識別する仕組みが必要
    - [[ページの行ベース言語モデル]]
    - 当初言語モデル的な形で学習で作ろうと考えていたが、サンプルとしてルールベースで書いたら意外とあっさりできた
- これは「エンジニアの知的生産術」で試してうまくいったが、「知の探検術」に使ったら全く本文が取れなかった
    - 前者は段落の頭が全角スペースでインデントされているが、後者はそうではないため
- ルールベース版を修正するより、機械学習にしようと考えた
    - 前後2行を加えた5行を入力とすることを想定して、ベースラインとして1行のものを作ったが、それで十分うまくいった
        - [[OCRゴミ掃除]]は別途必要そう
- 2018-10-18追記:
    - 「前後に行を加えた5行を入力とする」は[[textCNN]]的な発想だが、[[Transformer]]の方が色々面白そう
    - 行の特徴ベクトルは単語の[[埋め込みベクトル]]から作っているわけではない、同様にページの特徴ベクトルも作れるはず
        - [[目次の自動生成]]
![image](https://gyazo.com/696b82b79dce69112dd3cbaabe29cb23/thumb/1000)
- PDFから変換したテキストを一行1アイテムとして特徴量ベクトルに変換(output-features)
    - 行、特徴量、ラベル、空行、の4行セット
- 人間がそれを見て、適当にラベルを付与する
- その学習データをもとに能動学習(active-learning)
    - jsonで「ラベル付与済み特徴」「そのラベル」「識別に自信のない特徴量」を出力
        - 多クラス分類の能動学習なんで1位と2位の差でソートしている
    - 学習済みモデル
- マージ(merge)
    - 元データと、学習結果データをマージする
    - 実はoutput-featuresの特徴を使わずに新しく作り直すので、ここで新しい特徴を追加したりできる
    - 既存のデータとの紐付けにはpageRatio, lineRatioのペアが一意になることを利用する
    - マージ結果はhonbun_merged_features.txtに出力、diffして大きな問題がないことを確認してから手動でhonbun_features.txtにリネーム
- 本文出力(output-honbun)
    - 学習済みモデルと、特徴データから本文を抽出する。


最低限の手順
- --output-feature で特徴ベクトルを作成
- --output-honbun --use-model でintellitech/honbun_model.npzを指定
- でてきたhonbun.txtを見て、問題なさそうだったらOK

