---
title: "x/(x+1)"
---

$p = x/(x+c)$という確率pの推定について
話を簡単にするためにc=1とする。
- 1でない場合でもxとcを両方cで割ったものをそれぞれx, 1と呼べば良い。

$p = x/(x+1)$
xの値域は[0, infty)なので、これは(-infty, infty)であるyがx=exp(y)で写像されたものだとみなせる
- 対数の空間で、cで割るのは単なるlog(c)の平行移動に過ぎないから、議論を簡単にするためにcを原点に持ってきた

[[ロジスティック]][[シグモイド]]の定義は
$\sigma(s) = 1 / (1 + \exp(-s))$
だから
$\sigma(y) = 1 / (1 + \exp(-y))= \exp(y) / (\exp(y) + \exp(-y)\exp(y)) = x / (x + 1)$
つまり$x/(x+1)$は、シグモイドで確率推定していることに相当する。


![image](https://gyazo.com/eac341e8d11d062e3db5091a409083d1/thumb/1000)
- 図の分布関数 $p(y<Y)$ は $p(Y<y)$の間違い
- yの空間で、あるベル状の分布をとる確率変数Yがあった時にそれの[[累積分布関数]]はS字カーブになる
- このS字カーブをフィッティングすることで確率分布のパラメータを求めたい
    - ロジスティックシグモイドでやるアプローチ
    - ベル状の分布を正規分布にして、そのS字の密度関数を使うアプローチ
- x/(x+c)で確率を推定するのはロジスティックシグモイドを使うアプローチに対応する
    - [[ロジスティック回帰]]

シグモイドに帰着すると何が嬉しいのか
- シグモイドは各種の確率推定モデルで-infty~inftyの値を確率値に写像する目的で一般的に使われている
- 例えばロジスティック回帰は、入力ベクトルに対して重みのベクトルを内積して、得られた-infty~inftyの値をシグモイドに突っ込むことで確率推定をする
- なので「〜という式で確率推定をしている」って聞いた時に「それは既知の確率推定モデルの特殊ケースなのではないか」と考えたわけ
- シグモイドに帰着すると何が嬉しいかというと、それはつまり確率値を出力するロジスティック回帰やニューラルネットと対応するので、今フィーリングで決めている「重み」のパラメータを、適切な教師データを用意すれば学習によって調整できるわけ
- 推測モデルの精度が高まることが重要ならそのアプローチが取れるし、別に重要でないならorデータがないなら今の決め打ちのままで良い、と選択肢を増やすことができる

関連
- [[シグモイドとソフトマックスの関係]]


-----
イマイチだったアプローチ
[オッズ比 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%83%E3%82%BA%E6%AF%94)
$q/(1 − q)$
値域は[0, infty)
$x = q/(1 − q)$
とするなら
$y = \log(q) - \log(1-q)$


単語の出現回数がx、全単語の出現回数が$S = \sum x$である場合に、出現頻度を素朴に考えるとx/Sなんだけど
それだと学習データに出現しなかった単語は頻度0ってなっちゃうから分母に1を足す
既知単語の出現頻度が$x/(S+1)$で未知語の出現頻度が $1/(S+1)$ になる
単語が1種類しかない特殊ケースにおいては$S = \sum x = x$だから
その単語のスムージング後の出現頻度は$x/(x+1)$になる。
いわゆる加算スムージングではこれがさらに修正されて
既知単語の出現頻度が$(x + 1) /(S+c)$で未知語の出現頻度が $1 /(S+c)$ になる
ここでcは語彙数+1

