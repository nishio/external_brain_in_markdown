---
title: "一度だけ見たものは未知"
---


> [hillbig](https://twitter.com/hillbig/status/1729634555609391139/history) [[LLM]]はたとえ訓練データが正しくても[[Hallucination]]（幻覚）を起こす。これは正しく較正された予測分布は[[Good-Turing推定]]と同じく、訓練中に1度だけ観測した事実と同じ確率を未知の事実に割り振るためである。幻覚低減には事前学習後、別の学習が必要であることを示す
>  [[Calibrated Language Models Must Hallucinate]]
>  Recent language models have a mysterious tendency to generate false but plausible-sounding text. Such "hallucinations" are an obstacle to the usability of language-based AI systems and can harm...
- [Good–Turing推定 - Wikipedia](https://ja.wikipedia.org/wiki/Good%E2%80%93Turing%E6%8E%A8%E5%AE%9A)

[[一度しか経験してないことに意味を見出してしまう]][[ホモサピエンス]]が[[キャリブレーション]]不足なだけ、という解釈もできる

[[人間のバグ]]
