---
title: "注意機構"
---

[[注意]]([[Attention]])
2018年現在の一般化
- $\mathrm{Attention}(query, Keys, Values) = \mathrm{Normalize}(F(query, Keys)) \cdot Values$
- queryと複数のkeyの束であるKeysがある
- queryとKeysを引数にとってそれぞれのkeyに対する注意の強さを返す関数Fがある
- その結果を何らかの方法で合計が1になるように正規化して注意強度を得る(だいたいsoftmaxだが see [[ハード注意機構]])
- その注意強度でValuesを重み付け平均する

- 図解
    - ![image](https://gyazo.com/211618e709ff284a379c5c2f502934da/thumb/1000)

- FはKeyの個数を知らない。$F(query, Key)$はKeyのshapeに依存しない。
    - 数学語でどう表現するのが良いかわからない。
    - 一つのqueryと一つのkeyを受け取る関数fがあって`[f(query, key) for key in Keys]`

2014年 [[加法注意]] [1409.0473 Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- Func := Feed-Forward Network
- $Attention(query, Key, Value) = Softmax(FFN(concat(query, Key))) \cdot Value$
- > By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.
- RNNの隠れ状態は固定長のベクトルで、文章全体のデータをそこに詰め込んで覚えておくのは負担
- 注意機構は任意長のデータから情報を取り出すことができるのでその負担を軽減する
    - ![image](https://gyazo.com/dab69f04c581681e9c3c543b92633ef5/thumb/1000)


2015年 [[内積注意]] [1508.04025 Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
- queryとkeyを単に内積したもので良いという割り切り
- $Attention(query, Key, Value) = Softmax(query \cdot Key) \cdot Value$
- もちろんこの内積は論文によっては行列積で表現されたりしている
- 関連 [[双線形]]
![image](https://gyazo.com/1db88d9a61dcce7af368f0d0e594b3f1/thumb/1000)

[[ソースターゲット注意]]と[[自己注意]]([2017](https://arxiv.org/abs/1703.03130))
- 当初、注意機構はRNNと組み合わせて使うことが想定されていた
- Encoder-Decoder構成でのEncoderの隠れ状態を保存しておき、注意機構によってその隠れ状態の中から選択する
- この構成だとKey, ValueはEncoderから、queryはDecoderから来る
- この種の構成を[[ソースターゲット注意]]と呼ぶ
    - ([Sequence Generation with Target Attention](http://ecmlpkdd2017.ijs.si/papers/paperID307.pdf)(2017)でsource-target attentionとtarget-target attentionという形で比較議論されている)
- KとVを合わせてMemoryと呼ぶ
- 対義語が[[自己注意]](Self-attention)
    - こちらはKey, Value, queryすべてが自己である…いや、その定義は抽象度が釣り合ってないから…
    - いずれもっと良い用語に分化していくかもしれない
    - 今のところ一つの実装例としては「全部下位レイヤーから来る」
        - この形だと[[CNN]]の発展形
        - 固定長の入力しか受け取れなかったCNNが不定長の入力を受け取れる[[注意機構]]で置き換えられた
        - この置き換えに関する解説: [[CNNと自己注意]]


-----
古い解説
- この解説は[[RNN]]を暗黙に想定していて一般化されていない:
    - 過去の[[隠れ状態]]を取っておく
    - 「今の隠れ状態とその隠れ状態」から適当な注意の強さを表すスカラーを作る
    - そのスカラーを合計1に正規化する
    - それぞれの隠れ状態の重み付き平均に使う
        - 過去の隠れ状態ではなく出力層の値を使う手もある