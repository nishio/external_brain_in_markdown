---
title: "Information-Geometric Optimization"
---

2019-07-29
æ¢ç´¢ç©ºé–“ã§å®šç¾©ã•ã‚ŒãŸç›®çš„é–¢æ•°ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§å®šç¾©ã•ã‚ŒãŸé–¢æ•°ã«å¤‰æ›ã™ã‚‹éš›ã«ã€å¾“æ¥ã¯ä¸€èˆ¬çš„ã«æœŸå¾…å€¤ã‚’å–ã‚‹æ–¹æ³•ãŒä½¿ã‚ã‚Œã‚Œã„ãŸãŒã€[[å¹³å‡ã¯ç›®çš„é–¢æ•°ã®å˜èª¿å¢—åŠ ãªé–¢æ•°ã«ã‚ˆã‚‹å†™åƒã«å¯¾ã—ã¦ä¸å¤‰é‡ã§ã¯ãªã„]]ã®ã§quantileã‚’ç”¨ã„ãŸå®šç¾©ã«å¤‰ãˆãŸ
- å‹‰å¼·ä¼šç™ºè¡¨è³‡æ–™: [[Information-Geometric Optimizationã®ç´¹ä»‹]]

![image](https://gyazo.com/0a5fbd0a8c79402a155318bd154447b5/thumb/1000)

[http://www.jmlr.org/papers/volume18/14-467/14-467.pdf](http://www.jmlr.org/papers/volume18/14-467/14-467.pdf)
- We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space ğ‘‹ into a continuous-time black-box optimization method on ğ‘‹, the [[information-geometric optimization]] ([[IGO]]) method.
- [[Invariance]] as a major design principle keeps the number of arbitrary choices to a minimum.
- The resulting [[IGO flow]] is the flow of an ordinary differential equation conducting the [[natural gradient ascent]] of an adaptive, time-dependent transformation of the objective function.
- It makes no particular assumptions on the objective function to be optimized.
- The IGO method produces explicit [[IGO algorithms]] through time discretization.
- It naturally recovers versions of known algorithms and offers a systematic way to derive new ones.
    - In continuous search spaces, IGO algorithms take a form related to [[natural evolution strategies]] ([[NES]]).
    - The [[cross-entropy method]] is recovered in a particular case with a large time step, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML).
    - When applied to the family of Gaussian distributions on $R^d$, the IGO framework recovers a version of the well-known [[CMA-ES]] algorithm and of [[xNES]].
    - For the family of [[Bernoulli distributions]] on $\{0, 1\}^d$, we recover the [[seminal PBIL]] algorithm and [[cGA]].
    - For the distributions of restricted Boltzmann machines, we naturally obtain a novel algorithm for discrete optimization on $\{0, 1\}^d$.
- All these algorithms are natural instances of, and unified under, the single information-geometric optimization framework.
- The IGO method achieves, thanks to its intrinsic formulation, maximal invariance properties:
    - invariance under reparametrization of the search space ğ‘‹,
    - under a change of parameters of the probability distribution,
    - and under increasing transformation of the function to be optimized.
    - The latter is achieved through an adaptive, quantile-based formulation of the objective.
- Theoretical considerations strongly suggest that IGO algorithms are essentially characterized by a minimal change of the distribution over time. Therefore they have minimal loss in diversity through the course of optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. As a simple consequence, IGO seems to provide, from information theory, an elegant way to simultaneously explore several valleys of a fitness landscape in a single run.

- search space: X
    - æœ‰é™ã€é›¢æ•£ç„¡é™ã€é€£ç¶šã®ã©ã‚Œã§ã‚‚ã‚ã‚Šå¾—ã‚‹
- objective function$f: X \to R$
- black-box senario: f(x)ã‚’æ±‚ã‚ã‚‹ä»¥å¤–ã®fã«é–¢ã™ã‚‹çŸ¥è­˜ã¯å¾—ã‚‰ã‚Œãªã„ã‚‚ã®ã¨ã™ã‚‹
- invariance
    - extends performance observed on a single function to an entire associated invariance class
    - generalizes from a single problem to a class of problems
    - ??
    - Thus it hopefully provides better robustness w.r.t. changes in the presentation of a problem.
- For continuous search spaces, invariance under translation of the coordinate system is standard in optimization.
    - <img src='https://scrapbox.io/api/pages/nishio/nishio/icon' alt='nishio.icon' height="19.5"/>ã‚ãƒ¼ã€ä¸å¤‰é‡ã®æ¦‚å¿µã‹
- Invariance under general affine-linear changes of the coordinates has been
    - Nelder-Mead Downhill Simplex method (Nelder and Mead, 1965),
    - quasi-Newton methods (Deuflhard, 2011)
    - [[covariance matrix adaptation evolution strategy]], [[CMA-ES]] (Hansen and Ostermeier, 2001; Hansen and Auger, 2014)
- ã“ã‚Œã‚‰ã¯æ¢ç´¢ç©ºé–“ã«å¯¾ã™ã‚‹ä¸å¤‰é‡
- ä¸€æ–¹ã€ç›®çš„é–¢æ•°ã«å¯¾ã™ã‚‹å˜èª¿å¢—åŠ ãªå¤‰æ›

- iteratively update a probability distribution $ğ‘ƒ_\theta$ defined on ğ‘‹, parametrized by a set of parameters $\theta$
    - ã“ã®ç¢ºç‡åˆ†å¸ƒã¯è§£ã«å¯¾ã™ã‚‹[[ä¿¡å¿µ]]ã¨è¨€ãˆã‚‹([[POMDP]]ãªã©ã§ã®æ„å‘³ã®)

- [[estimation of distribution algorithms]] ([[EDA]])
    - æ–¹æ³•1: cross-entropy method consists in taking ğœƒ minimizing the Kullbackâ€“Leibler divergence between ğ‘ƒğœƒ and the indicator of the best points according to ğ‘“
        - indicator of the best points according to f ã¨ã¯ï¼Ÿ
    - æ–¹æ³•2: one can transfer the objective function ğ‘“ to the space of parameters ğœƒ by taking the average of ğ‘“ under ğ‘ƒğœƒ, seen as a function of ğœƒ.
        - This average is a new function from an Euclidian space to R and is minimal when ğ‘ƒğœƒ is concentrated on minima of ğ‘“.
            - Î¸ã¯1æ¬¡å…ƒã¨ã„ã†æƒ³å®šã‹ãªï¼Ÿ
            - ãã†ã§ãªãã¦ã‚‚å‹¾é…æ³•ã§è§£ã‘ã‚‹ã‚ˆã­
    - ç”˜åˆ©ã®è‡ªç„¶å‹¾é…æ³•ã®è©±
        - æ™®é€šã®å‹¾é…ã‚ˆã‚ŠåŠ¹ç‡çš„
- Euler time discretization
    - ã‚ªã‚¤ãƒ©ãƒ¼æ³•
    - ãƒ«ãƒ³ã‚²ã‚¯ãƒƒã‚¿ã®æ‰‹å‰

- å¤šåˆ†(1)ã®ä»˜ã„ã¦ã„ã‚‹å ´æ‰€ã€1ã¤ãšã‚Œã¦ã‚‹

- 2.2.1
    - fã‚’Xã§ã¯ãªãthetaã®ä¸Šã®é–¢æ•°ã«ã™ã‚‹ã«ã¯thetaã§æœŸå¾…å€¤ã‚’å–ã‚‹ã®ãŒæ‰‹
    - ã ãŒ
        - æœŸå¾…å€¤ã¯æ¥µç«¯ãªå€¤ã«å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„
        - å˜èª¿å¢—åŠ ãªå¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰é‡ã§ã¯ãªã„
    - ã ã‹ã‚‰ç™¾åˆ†é‡ã‚’ä½¿ã†
        - ä¸­å¤®å€¤ã¯å˜èª¿å¢—åŠ ãªé–¢æ•°ã§å¤‰æ›ã—ã¦ã‚‚ä¸­å¤®å€¤ã ã‹ã‚‰ã­
