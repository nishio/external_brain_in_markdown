---
title: "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
---

2006.16236 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
[https://arxiv.org/abs/2006.16236](https://arxiv.org/abs/2006.16236)
この論文は、自己回帰的な生成タスクにおけるトランスフォーマーモデルの計算量とメモリ使用量を大幅に削減する新しい手法 "[[Linear Transformer]]" を提案しています。主な内容は以下の通りです。
- 行列積の結合則を利用して自己注意機構の計算を線形時間で行えるようにした。これにより、系列長に対して線形のメモリと計算量で自己注意を計算できる。
- 因果マスクを用いた自己回帰的なタスクでも、線形時間と一定メモリで計算できることを示した。これにより、トランスフォーマーをRNNのように扱え、自己回帰的な推論を高速に行えるようになった。
- 画像生成と音声認識の実験により、提案手法が従来のトランスフォーマーと同等の性能を維持しつつ、はるかに高速に推論できることを示した。特に画像生成では、最大4000倍の高速化を達成した。
- 線形トランスフォーマーを因果マスク付きでRNNとして定式化した。これは、トランスフォーマーとRNNの関係性の理解に役立つ。

全体として、線形トランスフォーマーは、自己回帰的な生成タスクにおけるトランスフォーマーの実用性を大きく向上させる手法といえます。計算量とメモリ使用量を系列長に対して線形にしたことで、長い系列を扱えるようになりました。

[[注意機構の計算量削減]]
[[Linear Attention]]
