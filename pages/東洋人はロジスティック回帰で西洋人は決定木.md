---
title: "東洋人はロジスティック回帰で西洋人は決定木"
---

![image](https://gyazo.com/65041f75b35b16a9903a8525291966c5/thumb/1000)
[[木を見る西洋人 森を見る東洋人]] p.161
図のターゲットはグループ1にいれるのがよいか、グループ2に入れるのがよいか。
東洋人はグループ1、西洋人はグループ2を選ぶ傾向がある。

[[家族的類似性]]の問題は4次元ベクトルと2つのベクトル集合の類似度 see [[ベクトルの類似度]]
それぞれの集合はラベル付きで事前に与えられているので[[教師あり学習]]
花弁、花序、葉、茎の特徴をターゲットを(1, 1, 1, 1)として表現する
グループ1
- (1, 1, 1, 0)
- (1, 0, 1, 0)
- (1, 1, 0, 0)
- (0, 1, 1, 0)
グループ2
- (0, 0, 1, 1)
- (0, 0, 0, 1)
- (0, 1, 0, 1)
- (1, 0, 0, 1)
軸をそれぞれ(a, b, c, d)と呼ぶことにする
西洋型識別はd軸以外の情報を捨てて識別している
東洋型識別は全ての軸を使う

[[ロジスティック回帰]]だと「確率55%でグループ1」と判断する
python

```
import numpy as np
from sklearn.linear_model import LogisticRegression

X = np.array([

 (1, 1, 1, 0),
 (1, 0, 1, 0),
 (1, 1, 0, 0),
 (0, 1, 1, 0),

 (0, 0, 1, 1),
 (0, 0, 0, 1),
 (0, 1, 0, 1),
 (1, 0, 0, 1)])

Y = np.array([0, 0, 0, 0, 1, 1,	1, 1])

m = LogisticRegression()
m.fit(X, Y)
```


:

```
In [3]: m.coef_
Out[3]: array([[-0.47815958, -0.47815958, -0.47815958,  1.16980067]])

In [4]: m.intercept_
Out[4]: array([0.08158937])

In [5]: m.predict_proba([(1, 1, 1, 1)])
Out[5]: array([[0.54564474, 0.45435526]])
```


coef_を見ると、a, b, c軸に関して-0.48(弱くグループ1を示唆)、d軸に関して+1.17(強くグループ2を示唆)
ロジスティック回帰はこれを足し合わせるのでa, b, cの軸での判断が勝ち、僅差でグループ1という判断を下す。

[[決定木]]だと100%グループ2と判定する
python

```
from sklearn.tree import DecisionTreeClassifier
m = DecisionTreeClassifier()
m.fit(X, Y)
```


:

```
In [7]: m.predict_proba([(1, 1, 1, 1)])
Out[7]: array([[0., 1.]])
```

理由は簡単で、学習過程で「各軸の中で一番綺麗に割り切れるのはどれか」を探し、当然「dで判断する」という結論を導くから。識別フェーズではd以外の軸を見ていない。
