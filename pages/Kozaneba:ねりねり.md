---
title: "Kozaneba:ねりねり"
---

サムネイル
![image](https://gyazo.com/38bc5e9288210af1f4c20ede4cae8eab/thumb/1000)


[[GPT4と話す思考の結節点2023-08-10]]

section1
![image](https://gyazo.com/8e4749205073a99e6e4b60b6a5bff418/thumb/1000)

section2
![image](https://gyazo.com/c542497037d757bae5ee517751c88862/thumb/1000)

section3
![image](https://gyazo.com/f50d32dae1d79bad1a374cda8faf3ce9/thumb/1000)


section4
![image](https://gyazo.com/c0cfcb910e8f61077c39c9db3f04955b/thumb/1000)

merge
![image](https://gyazo.com/76b7e09d8c09b4be131a22353ea99fa1/thumb/1000)
![image](https://gyazo.com/6c172bab13c04e2ec7b14f318a4bf13b/thumb/1000)

あつまれ〜

![image](https://gyazo.com/a822dd737784335a916bac60face5551/thumb/1000)

人間の会話
書き物
1次元
時系列
断片化
刻んで時系列の構造を壊す

トピック
散らばっている
時系列の構造を破壊しなければ
時間指向
トピック指向の新しい構造を作れない

Spaced Repetition
時間軸にも緩やかな価値

連想検索
2000次元
近接

Kozaneba
2次元
線を引く
破壊の苦痛をやわらげる

<img src='https://scrapbox.io/api/pages/nishio/gpt/icon' alt='gpt.icon' height="19.5"/>
この続きの部分では、人間のコミュニケーションや情報の構造についてのいくつかのキーポイントとコンセプトが示されています。以下、それぞれの要素を解釈してみます。

### 人間の会話と書き物
- 人間の会話や書き物は1次元の時系列で展開されるとされています。この線形的な構造は、情報を断片化し、特定のトピックやテーマ性を探求する際に構造を壊す必要があるかもしれません。

### トピックと時系列
- トピック指向と時系列、または時間指向の間の対立が強調されています。トピック指向の新しい構造を作るためには、時系列の構造を破壊しなければならない可能性が示唆されています。これは、情報の整理や解釈における課題となるかもしれません。

### Spaced Repetition
- このコンセプトは学習に関連しており、情報を一定の間隔で繰り返し復習する手法を指します。時間軸に沿ったこのアプローチには「緩やかな価値」があるとされています。おそらく、時間的に散らばっている情報を結びつけ、理解を深化させるのに役立つのではないでしょうか。

### 連想検索
- 「2000次元」という表現は、情報を多角的に関連づけるための高次元の検索や分析を指している可能性があります。多次元の分析は、情報の構造や関連性をより豊かに捉えるのに役立つでしょう。

### Kozaneba
- 前述の議論に続いて、Kozanebaの特性が再び触れられています。2次元の空間で情報を整理し、線を引くことで関連を表現することが強調されています。そして、このアプローチが「破壊の苦痛」を軽減するとされています。

### 総合的な分析
- 全体を通して、情報の整理と解釈、そして意味の創造に関連する様々な要素と課題が浮かび上がっています。時系列とトピック指向、1次元と多次元、断片化と統合など、これらの対立と相互作用が情報の理解と使用にどう影響するかを探求することは、コミュニケーションと知識管理の分野において興味深いテーマ性を提供するでしょう。

![image](https://gyazo.com/38bc5e9288210af1f4c20ede4cae8eab/thumb/1000)

「ねりねり」が「構造の破壊」かというと、そうではない
- 一度破壊した後、新しい構造が生み出される

なぜAI時代に古いのか
- コピー機すらなかった時代に生み出された[[こざね法]]や[[KJ法]]は古い
- 最も新しい[[考える花火]]にKozanebaは近いがそれでも何十年も古い

生成AIと連想検索が有用
- 実際この実験では[[西尾のベクトル検索]]で連想検索した結果を断片的メモとして渡して生成AIに文章を生成させている

Kozanebaは1次元の文章を刻んだ断片にした後、2次元空間に配置していく手法
連想検索の前段階のベクトル埋め込みとは、文章を2000次元の空間に配置していく手法
なだらかにつなぐことができるといいな
