---
title: "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance"
---

[[Proactive Agent]]
[Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance](https://arxiv.org/abs/2410.12361)

<img src='https://scrapbox.io/api/pages/nishio/o1 Pro/icon' alt='o1 Pro.icon' height="19.5"/>
# 論文の目的
- 従来のLLMエージェントは、人間から明示的な指示を受けてから動く「[[リアクティブ]]」型が中心だった。
- この論文では、ユーザの行動や環境の状況をモニタリングし、「今ユーザが必要としそうなタスク」を先読みして自発的に提案する「[[プロアクティブ]]」型のエージェントを目指す。

# 具体的なアプローチ
1. ### データ収集とパイプラインの構築
    - 実際のユーザの「コーディング」「文書作成」「日常生活」などでの操作履歴を集め、そこから環境・ユーザ行動のログ（イベント）を得る。
        - 大規模言語モデル（GPT-4 など）を使い、これらのログを基に仮想的な「環境ジム（Environment Gym）」を作る。
    - ジム上で、ユーザ役（User Agent）が行動を起こし、エージェントが先読み提案を行い、それをユーザ役が受け入れるか拒否するかをシミュレーションして大量の学習データを生成。

2. ### ProactiveBench の構築
    - 上記のパイプラインを用いて「ProactiveBench」というデータセットを作成（合計6,790イベント）。
        - トレーニング用に多様な「イベント履歴」と「先読みタスク提案（採用・却下）」を含む。
    - 実際のユーザの行動ログから作ったテストセット（233イベント）も用意。

3. ### Reward Model（報酬モデル）の導入
    - 人間アノテータが、エージェントが提案したタスクを「受け入れる（必要だった）」「拒否する（不要だった）」と判断したラベルを付与。
        - この人間の判断を模倣する報酬モデルを学習し、エージェントが提案したタスクの「妥当性（受け入れられるか）」を自動で判定。

4. ### 実験と評価
    - LLaMAやQwenなどのオープンソースLLMをベースに学習・微調整したところ、
        - 適切な先読みタスク提案のF1スコアが約55%→66%前後に向上。
    - GPT-4やClaudeといったクローズドソースLLMでも評価し、オープンソースLLMの微調整モデルが匹敵・上回る結果を得られたケースも示唆。

# 主な成果と意義
- プロアクティブな提案を行うためのデータ駆動型フレームワークを示し、ツールやデータセット（ProactiveBench）を公開。
- 報酬モデルを活用することで、自動的に「ユーザは本当に必要としているか」をフィードバックしながらエージェントを評価・改善可能にした。
- 実験結果では、先読み提案の精度が着実に上がるものの、「ユーザの必要のない場面で提案してしまう」ケース（False Alarm）の低減やタイミング調整はまだ課題。

# 今後の課題
- シミュレーション環境が限定されているため、さらに多様なシナリオへの拡張が必要。
- ユーザにとって不要な場面でも提案を行ってしまう問題を低減する、より精度の高いコンテキスト理解が課題。
- プライバシー・倫理面にも配慮しつつ、実際のユーザ行動との統合方法を検討する必要がある。

--

## 総括
この論文は、大規模言語モデルを「リアクティブ」な応答装置から「プロアクティブ」に先回りしてタスクを提案するアシスタントへと進化させるための枠組みを示しています。大量のシミュレーションデータを作る仕組みと、そのデータを用いた微調整・報酬モデル評価により、エージェントがユーザの行動を読み取り、有益なタイミングでタスク支援を提案できるようにする点が最大の特徴です。実験では大幅な性能向上が得られた一方、タイミングの微妙さや誤検出の多さなど、実運用における課題も提示されています。
